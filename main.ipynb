{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2kI_IpN__Pb1",
        "outputId": "fb2b1ad9-d36f-4b58-dfd7-b122ebce52a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (1.2.0)\n",
            "Requirement already satisfied: langgraph in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (1.0.5)\n",
            "Requirement already satisfied: bitsandbytes in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (0.49.0)\n",
            "Requirement already satisfied: peft in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (0.18.0)\n",
            "Requirement already satisfied: trl in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (0.26.2)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.1 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from langchain) (1.2.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from langchain) (2.12.5)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from langgraph) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from langgraph) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from langgraph) (0.3.1)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from langgraph) (3.6.0)\n",
            "Requirement already satisfied: torch<3,>=2.3 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from bitsandbytes) (2.9.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from bitsandbytes) (2.4.0)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: psutil in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from peft) (7.1.3)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from peft) (6.0.3)\n",
            "Requirement already satisfied: transformers in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from peft) (4.57.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from peft) (1.12.0)\n",
            "Requirement already satisfied: safetensors in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from peft) (0.7.0)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from peft) (0.36.0)\n",
            "Requirement already satisfied: datasets>=3.0.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from trl) (4.4.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from datasets>=3.0.0->trl) (3.20.1)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from datasets>=3.0.0->trl) (22.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from datasets>=3.0.0->trl) (0.4.0)\n",
            "Requirement already satisfied: pandas in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from datasets>=3.0.0->trl) (2.3.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from datasets>=3.0.0->trl) (2.32.5)\n",
            "Requirement already satisfied: httpx<1.0.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from datasets>=3.0.0->trl) (0.28.1)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from datasets>=3.0.0->trl) (0.70.18)\n",
            "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.5.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (9.1.2)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.12.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph) (1.12.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (3.11.5)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from torch<3,>=2.3->bitsandbytes) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: setuptools in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from torch<3,>=2.3->bitsandbytes) (80.9.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from transformers->peft) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from transformers->peft) (0.22.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (3.13.2)\n",
            "Requirement already satisfied: anyio in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (4.12.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (1.0.9)\n",
            "Requirement already satisfied: idna in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=3.0.0->trl) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.6.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from pandas->datasets>=3.0.0->trl) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install langchain langgraph bitsandbytes peft trl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WyGzkc5ez3a"
      },
      "source": [
        "# Download e exploração inicial dos dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CuxJtGggAxGf",
        "outputId": "a4f2aed2-e044-4c0e-cb39-def01c5c543c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Campos disponíveis: ['QUESTION', 'CONTEXTS', 'LABELS', 'MESHES', 'YEAR', 'reasoning_required_pred', 'reasoning_free_pred', 'final_decision', 'LONG_ANSWER']\n",
            "\n",
            "============================================================\n",
            "Exploração de dados - PubMedQA\n",
            "============================================================\n",
            "\n",
            "Exemplo 1 | ID: 21645374\n",
            "------------------------------------------------------------\n",
            "Question: Do mitochondria play a role in remodelling lace plant leaves during programmed cell death?\n",
            "Context: Programmed cell death (PCD) is the regulated death of cells within an organism. The lace plant (Aponogeton madagascariensis) produces perforations in its leaves through PCD. The leaves of the plant consist of a latticework of longitudinal and transverse veins enclosing areoles. PCD occurs in the cel...\n",
            "Labels: ['BACKGROUND', 'RESULTS']\n",
            "Decision: yes\n",
            "Answer: Results depicted mitochondrial dynamics in vivo as PCD progresses within the lace plant, and highlight the correlation of this organelle with other organelles during developmental PCD. To the best of ...\n",
            "Meshes: ['Alismataceae', 'Apoptosis', 'Cell Differentiation', 'Mitochondria', 'Plant Leaves']\n",
            "Year: 2011\n",
            "Reasoning required pred: yes\n",
            "Reasoning free pred: yes\n",
            "\n",
            "Exemplo 2 | ID: 16418930\n",
            "------------------------------------------------------------\n",
            "Question: Landolt C and snellen e acuity: differences in strabismus amblyopia?\n",
            "Context: Assessment of visual acuity depends on the optotypes used for measurement. The ability to recognize different optotypes differs even if their critical details appear under the same visual angle. Since optotypes are evaluated on individuals with good visual acuity and without eye disorders, differenc...\n",
            "Labels: ['BACKGROUND', 'PATIENTS AND METHODS', 'RESULTS']\n",
            "Decision: no\n",
            "Answer: Using the charts described, there was only a slight overestimation of visual acuity by the Snellen E compared to the Landolt C, even in strabismus amblyopia. Small differences in the lower visual acui...\n",
            "Meshes: ['Adolescent', 'Adult', 'Aged', 'Aged, 80 and over', 'Amblyopia', 'Cataract', 'Child', 'Eye Diseases', 'Female', 'Humans', 'Male', 'Middle Aged', 'Reference Values', 'Refractive Errors', 'Reproducibility of Results', 'Retinal Diseases', 'Strabismus', 'Vision Tests', 'Visual Acuity']\n",
            "Year: 2006\n",
            "Reasoning required pred: no\n",
            "Reasoning free pred: no\n",
            "\n",
            "Exemplo 3 | ID: 9488747\n",
            "------------------------------------------------------------\n",
            "Question: Syncope during bathing in infants, a pediatric form of water-induced urticaria?\n",
            "Context: Apparent life-threatening events in infants are a difficult and frequent problem in pediatric practice. The prognosis is uncertain because of risk of sudden infant death syndrome. Eight infants aged 2 to 15 months were admitted during a period of 6 years; they suffered from similar maladies in the b...\n",
            "Labels: ['BACKGROUND', 'CASE REPORTS']\n",
            "Decision: yes\n",
            "Answer: \"Aquagenic maladies\" could be a pediatric form of the aquagenic urticaria....\n",
            "Meshes: ['Baths', 'Histamine', 'Humans', 'Infant', 'Syncope', 'Urticaria', 'Water']\n",
            "Year: 1997\n",
            "Reasoning required pred: yes\n",
            "Reasoning free pred: yes\n",
            "\n",
            "\n",
            "Total de registros: 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'pubmedqa' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/pubmedqa/pubmedqa.git\n",
        "\n",
        "import json\n",
        "\n",
        "file_path = 'pubmedqa/data/ori_pqal.json'\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "sample_key = list(data.keys())[0]\n",
        "print(f\"\\nCampos disponíveis: {list(data[sample_key].keys())}\\n\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Exploração de dados - PubMedQA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, key in enumerate(list(data.keys())[:3]):\n",
        "    item = data[key]\n",
        "    \n",
        "    print(f\"\\nExemplo {i+1} | ID: {key}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"Question: {item.get('QUESTION', 'N/A')}\")\n",
        "    \n",
        "    context = \" \".join(item.get('CONTEXTS', []))\n",
        "    print(f\"Context: {context[:300]}...\")\n",
        "    \n",
        "    print(f\"Labels: {item.get('LABELS', 'N/A')}\")\n",
        "    print(f\"Decision: {item.get('final_decision', 'N/A')}\")\n",
        "    print(f\"Answer: {item.get('LONG_ANSWER', 'N/A')[:200]}...\")\n",
        "    print(f\"Meshes: {item.get('MESHES', 'N/A')}\")\n",
        "    print(f\"Year: {item.get('YEAR', 'N/A')}\")\n",
        "    print(f\"Reasoning required pred: {item.get('reasoning_required_pred', 'N/A')}\")\n",
        "    print(f\"Reasoning free pred: {item.get('reasoning_free_pred', 'N/A')}\")\n",
        "\n",
        "print(f\"\\n\\nTotal de registros: {len(data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gN3SxdO2mmXH"
      },
      "source": [
        "# Divisão dos dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXB173BcmqjF",
        "outputId": "e47a0180-11d4-4a9a-9220-26df93e1832f"
      },
      "outputs": [],
      "source": [
        "!cd pubmedqa/preprocess && python split_dataset.py pqal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRFH2sIYarWe"
      },
      "source": [
        "# Pré-processamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4fd06e94",
        "outputId": "b29258b7-ee04-4a0c-963e-1fd25652298f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processando dados...\n",
            "Test set: 500 registros\n",
            "Fold 0: 50 registros\n",
            "Fold 1: 50 registros\n",
            "Fold 2: 50 registros\n",
            "Fold 3: 50 registros\n",
            "Fold 4: 50 registros\n",
            "Fold 5: 50 registros\n",
            "Fold 6: 50 registros\n",
            "Fold 7: 50 registros\n",
            "Fold 8: 50 registros\n",
            "Fold 9: 50 registros\n",
            "\n",
            "Processamento concluído\n",
            "Arquivos salvos em: data_processed/\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "import unicodedata\n",
        "import os\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    \n",
        "    text = unicodedata.normalize('NFKC', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = re.sub(r'[^\\w\\s.,?!():%\\-\\+]', '', text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "def process_file(input_path, output_path):\n",
        "    with open(input_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    \n",
        "    processed_data = {}\n",
        "    \n",
        "    for key, item in data.items():\n",
        "        question = preprocess_text(item.get('QUESTION', ''))\n",
        "        \n",
        "        context_raw = \" \".join(item.get('CONTEXTS', []))\n",
        "        context = preprocess_text(context_raw)\n",
        "        \n",
        "        decision = item.get('final_decision', 'N/A').upper()\n",
        "        long_answer = item.get('LONG_ANSWER', '')\n",
        "        answer = preprocess_text(f\"Decisão: {decision}. Justificativa: {long_answer}\")\n",
        "        \n",
        "        processed_data[key] = {\n",
        "            \"QUESTION\": question,\n",
        "            \"CONTEXTS\": context,\n",
        "            \"FINAL_ANSWER\": answer,\n",
        "            \"YEAR\": item.get('YEAR', 'N/A')\n",
        "        }\n",
        "    \n",
        "    with open(output_path, 'w', encoding='utf-8') as f_out:\n",
        "        json.dump(processed_data, f_out, indent=4, ensure_ascii=False)\n",
        "    \n",
        "    return len(processed_data)\n",
        "\n",
        "os.makedirs('data_processed', exist_ok=True)\n",
        "\n",
        "print(\"Processando dados...\")\n",
        "\n",
        "total = process_file(\n",
        "    'pubmedqa/data/test_set.json',\n",
        "    'data_processed/test_set_preprocessed.json'\n",
        ")\n",
        "print(f\"Test set: {total} registros\")\n",
        "\n",
        "for i in range(10):\n",
        "    os.makedirs(f'data_processed/pqal_fold{i}', exist_ok=True)\n",
        "    total = process_file(\n",
        "        f'pubmedqa/data/pqal_fold{i}/dev_set.json',\n",
        "        f'data_processed/pqal_fold{i}/dev_set_preprocessed.json'\n",
        "    )\n",
        "    print(f\"Fold {i}: {total} registros\")\n",
        "\n",
        "print(\"\\nProcessamento concluído\")\n",
        "print(\"Arquivos salvos em: data_processed/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ieOgONUhOkj"
      },
      "source": [
        "# Anonimização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xmlGwHjhWQJ",
        "outputId": "b3a01cb8-3b36-4f00-bfe2-c01234470f7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Anonimizando dados...\n",
            "Test set: 435 registros\n",
            "Fold 0: 50 registros\n",
            "Fold 1: 50 registros\n",
            "Fold 2: 49 registros\n",
            "Fold 3: 49 registros\n",
            "Fold 4: 50 registros\n",
            "Fold 5: 50 registros\n",
            "Fold 6: 48 registros\n",
            "Fold 7: 50 registros\n",
            "Fold 8: 50 registros\n",
            "Fold 9: 50 registros\n",
            "\n",
            "Anonimização concluída\n",
            "Arquivos salvos em: data_anonymized/\n",
            "\n",
            "Exemplo de dado anonimizado:\n",
            "ID: HOSP_REG_1237\n",
            "Question: Is anorectal endosonography valuable in dyschesia?...\n",
            "Original ID: [OCULTADO_POR_SEGURANCA]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "import os\n",
        "\n",
        "def anonymize_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    \n",
        "    text = re.sub(r'(Dr\\.|Dra\\.|Doctor|Prof\\.|MD)\\s+[A-Z][a-z]+(\\s+[A-Z][a-z]+)?', '[NOME_PROFISSIONAL]', text)\n",
        "    text = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', '[CONTATO_EMAIL]', text)\n",
        "    \n",
        "    locations = r'(Israel|Denmark|Chile|Texas|France|United Kingdom|UK|USA|Pakistan|Karachi|Jordan|Japan|Australia|North Carolina|Washington)'\n",
        "    text = re.sub(locations, '[LOCALIZACAO_RESTRITA]', text, flags=re.IGNORECASE)\n",
        "    \n",
        "    text = re.sub(r'\\b\\d{6,}\\b', '[ID_RESTRITO]', text)\n",
        "    text = re.sub(r'\\b(19|20)\\d{2}\\b', '[ANO]', text)\n",
        "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '[URL_RESTRITA]', text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "def anonymize_file(input_path, output_path):\n",
        "    with open(input_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    \n",
        "    anonymized = {}\n",
        "    \n",
        "    for key, item in data.items():\n",
        "        new_id = f\"HOSP_REG_{key[:4]}\"\n",
        "        \n",
        "        anonymized[new_id] = {\n",
        "            \"QUESTION\": anonymize_text(item.get('QUESTION', '')),\n",
        "            \"CONTEXTS\": anonymize_text(item.get('CONTEXTS', '')),\n",
        "            \"FINAL_ANSWER\": anonymize_text(item.get('FINAL_ANSWER', '')),\n",
        "            \"ORIGINAL_ID\": \"[OCULTADO_POR_SEGURANCA]\"\n",
        "        }\n",
        "    \n",
        "    with open(output_path, 'w', encoding='utf-8') as f_out:\n",
        "        json.dump(anonymized, f_out, indent=4, ensure_ascii=False)\n",
        "    \n",
        "    return len(anonymized)\n",
        "\n",
        "os.makedirs('data_anonymized', exist_ok=True)\n",
        "\n",
        "print(\"Anonimizando dados...\")\n",
        "\n",
        "total = anonymize_file(\n",
        "    'data_processed/test_set_preprocessed.json',\n",
        "    'data_anonymized/test_set_anonymized.json'\n",
        ")\n",
        "print(f\"Test set: {total} registros\")\n",
        "\n",
        "for i in range(10):\n",
        "    os.makedirs(f'data_anonymized/pqal_fold{i}', exist_ok=True)\n",
        "    total = anonymize_file(\n",
        "        f'data_processed/pqal_fold{i}/dev_set_preprocessed.json',\n",
        "        f'data_anonymized/pqal_fold{i}/dev_set_anonymized.json'\n",
        "    )\n",
        "    print(f\"Fold {i}: {total} registros\")\n",
        "\n",
        "print(\"\\nAnonimização concluída\")\n",
        "print(\"Arquivos salvos em: data_anonymized/\")\n",
        "\n",
        "with open('data_anonymized/test_set_anonymized.json', 'r', encoding='utf-8') as f:\n",
        "    sample = json.load(f)\n",
        "    first_key = list(sample.keys())[0]\n",
        "    print(f\"\\nExemplo de dado anonimizado:\")\n",
        "    print(f\"ID: {first_key}\")\n",
        "    print(f\"Question: {sample[first_key]['QUESTION'][:100]}...\")\n",
        "    print(f\"Original ID: {sample[first_key]['ORIGINAL_ID']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Análise de qualidade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analisando qualidade dos dados...\n",
            "\n",
            "Problemas encontrados:\n",
            "\n",
            "Distribuição - test set:\n",
            "  YES: 238 (54.7%)\n",
            "  NO: 176 (40.5%)\n",
            "  MAYBE: 21 (4.8%)\n",
            "\n",
            "Distribuição - folds:\n",
            "  YES: 244 (56.1%)\n",
            "  NO: 176 (40.5%)\n",
            "  MAYBE: 15 (3.4%)\n",
            "\n",
            "Total de registros:\n",
            "  Test: 435\n",
            "  Folds: 435\n",
            "  Total: 870\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "def analyze_quality(data):\n",
        "    issues = {\n",
        "        'question_vazia': [],\n",
        "        'context_vazio': [],\n",
        "        'answer_vazia': [],\n",
        "        'answer_muito_curta': [],\n",
        "        'context_muito_curto': []\n",
        "    }\n",
        "    \n",
        "    for key, item in data.items():\n",
        "        if not item.get('QUESTION', '').strip():\n",
        "            issues['question_vazia'].append(key)\n",
        "        \n",
        "        if not item.get('CONTEXTS', '').strip():\n",
        "            issues['context_vazio'].append(key)\n",
        "        \n",
        "        if not item.get('FINAL_ANSWER', '').strip():\n",
        "            issues['answer_vazia'].append(key)\n",
        "        \n",
        "        if len(item.get('FINAL_ANSWER', '')) < 50:\n",
        "            issues['answer_muito_curta'].append(key)\n",
        "        \n",
        "        if len(item.get('CONTEXTS', '')) < 100:\n",
        "            issues['context_muito_curto'].append(key)\n",
        "    \n",
        "    return issues\n",
        "\n",
        "def extract_decision(answer):\n",
        "    answer_upper = answer.upper()\n",
        "    if 'YES' in answer_upper or 'SIM' in answer_upper:\n",
        "        return 'YES'\n",
        "    elif 'NO' in answer_upper or 'NÃO' in answer_upper or 'NAO' in answer_upper:\n",
        "        return 'NO'\n",
        "    elif 'MAYBE' in answer_upper or 'TALVEZ' in answer_upper:\n",
        "        return 'MAYBE'\n",
        "    return 'UNKNOWN'\n",
        "\n",
        "def analyze_distribution(data):\n",
        "    distribution = Counter()\n",
        "    \n",
        "    for key, item in data.items():\n",
        "        decision = extract_decision(item.get('FINAL_ANSWER', ''))\n",
        "        distribution[decision] += 1\n",
        "    \n",
        "    return distribution\n",
        "\n",
        "print(\"Analisando qualidade dos dados...\")\n",
        "\n",
        "with open('data_anonymized/test_set_anonymized.json', 'r', encoding='utf-8') as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "issues = analyze_quality(test_data)\n",
        "\n",
        "print(\"\\nProblemas encontrados:\")\n",
        "for issue_type, ids in issues.items():\n",
        "    if ids:\n",
        "        print(f\"  {issue_type}: {len(ids)} registros\")\n",
        "\n",
        "print(\"\\nDistribuição - test set:\")\n",
        "dist_test = analyze_distribution(test_data)\n",
        "for cls, count in dist_test.items():\n",
        "    pct = (count / len(test_data)) * 100\n",
        "    print(f\"  {cls}: {count} ({pct:.1f}%)\")\n",
        "\n",
        "print(\"\\nDistribuição - folds:\")\n",
        "all_folds = {}\n",
        "for i in range(10):\n",
        "    with open(f'data_anonymized/pqal_fold{i}/dev_set_anonymized.json', 'r', encoding='utf-8') as f:\n",
        "        fold_data = json.load(f)\n",
        "        all_folds.update(fold_data)\n",
        "\n",
        "dist_folds = analyze_distribution(all_folds)\n",
        "for cls, count in dist_folds.items():\n",
        "    pct = (count / len(all_folds)) * 100\n",
        "    print(f\"  {cls}: {count} ({pct:.1f}%)\")\n",
        "\n",
        "print(f\"\\nTotal de registros:\")\n",
        "print(f\"  Test: {len(test_data)}\")\n",
        "print(f\"  Folds: {len(all_folds)}\")\n",
        "print(f\"  Total: {len(test_data) + len(all_folds)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validação de consistência\n",
            "\n",
            "Registro: HOSP_REG_1237\n",
            "Question: Is anorectal endosonography valuable in dyschesia?...\n",
            "Context: 1231 caracteres\n",
            "Answer: Decisão: YES. Justificativa: Linear anorectal endosonography demonstrated incomplete or even absent relaxation of the anal sphincter and the m. pubore...\n",
            "------------------------------------------------------------\n",
            "Registro: HOSP_REG_2616\n",
            "Question: Is there a connection between sublingual varices and hypertension?...\n",
            "Context: 1667 caracteres\n",
            "Answer: Decisão: YES. Justificativa: An association was found between sublingual varices and hypertension. Examining the lateral borders of the tongue is easi...\n",
            "------------------------------------------------------------\n",
            "Registro: HOSP_REG_1910\n",
            "Question: Are home sampling kits for sexually transmitted infections acceptable among men who have sex with me...\n",
            "Context: 1224 caracteres\n",
            "Answer: Decisão: MAYBE. Justificativa: The widespread acceptability of using HSKs for the diagnosis of STIs could have important public health impacts in term...\n",
            "------------------------------------------------------------\n",
            "\n",
            "Todas as amostras validadas\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "def validate_consistency(data, max_samples=5):\n",
        "    print(\"Validação de consistência\\n\")\n",
        "    \n",
        "    inconsistencies = []\n",
        "    \n",
        "    for key, item in list(data.items())[:max_samples]:\n",
        "        question = item.get('QUESTION', '')\n",
        "        context = item.get('CONTEXTS', '')\n",
        "        answer = item.get('FINAL_ANSWER', '')\n",
        "        \n",
        "        has_question_mark = '?' in question\n",
        "        valid_context = len(context) > 50\n",
        "        valid_answer = len(answer) > 30\n",
        "        \n",
        "        if not (has_question_mark and valid_context and valid_answer):\n",
        "            inconsistencies.append({\n",
        "                'id': key,\n",
        "                'has_question_mark': has_question_mark,\n",
        "                'valid_context': valid_context,\n",
        "                'valid_answer': valid_answer\n",
        "            })\n",
        "        \n",
        "        print(f\"Registro: {key}\")\n",
        "        print(f\"Question: {question[:100]}...\")\n",
        "        print(f\"Context: {len(context)} caracteres\")\n",
        "        print(f\"Answer: {answer[:150]}...\")\n",
        "        print(\"-\" * 60)\n",
        "    \n",
        "    return inconsistencies\n",
        "\n",
        "with open('data_anonymized/test_set_anonymized.json', 'r', encoding='utf-8') as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "inconsistencies = validate_consistency(test_data, max_samples=3)\n",
        "\n",
        "if inconsistencies:\n",
        "    print(f\"\\nInconsistências encontradas: {len(inconsistencies)}\")\n",
        "else:\n",
        "    print(f\"\\nTodas as amostras validadas\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Formatando para fine-tuning...\n",
            "Test set: 435 exemplos\n",
            "Fold 0: 50 exemplos\n",
            "Fold 1: 50 exemplos\n",
            "Fold 2: 49 exemplos\n",
            "Fold 3: 49 exemplos\n",
            "Fold 4: 50 exemplos\n",
            "Fold 5: 50 exemplos\n",
            "Fold 6: 48 exemplos\n",
            "Fold 7: 50 exemplos\n",
            "Fold 8: 50 exemplos\n",
            "Fold 9: 50 exemplos\n",
            "\n",
            "Formatação concluída\n",
            "Formato: JSONL conversacional\n",
            "Estrutura: system + user + assistant\n",
            "Arquivos salvos em: data_curated/\n",
            "\n",
            "Exemplo:\n",
            "{\n",
            "  \"id\": \"HOSP_REG_1237\",\n",
            "  \"messages\": [\n",
            "    {\n",
            "      \"role\": \"system\",\n",
            "      \"content\": \"Você é um assistente médico especializado. Responda às perguntas baseando-se nas evidências científicas fornecidas no contexto.\"\n",
            "    },\n",
            "    {\n",
            "      \"role\": \"user\",\n",
            "      \"content\": \"Contexto médico: Dyschesia can be provoked by inappropriate defecation movements. The aim of this prospective study was to demonstrate dysfunction of the anal sphincter andor the musculus (m.) puborectalis in patients with dyschesia using anorectal endosonography. Twenty consecutive patients with a medical history of dyschesia and a control group of 20 healthy subjects underwent linear anorectal endosonography (Toshiba models IUV 5060 and PVL-625 RT). In both groups, the dimensions of the anal sphincter and the m. puborec...\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "def format_for_finetuning(data, output_path):\n",
        "    formatted = []\n",
        "    \n",
        "    for key, item in data.items():\n",
        "        question = item.get('QUESTION', '')\n",
        "        context = item.get('CONTEXTS', '')\n",
        "        answer = item.get('FINAL_ANSWER', '')\n",
        "        \n",
        "        example = {\n",
        "            \"id\": key,\n",
        "            \"messages\": [\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"Você é um assistente médico especializado. Responda às perguntas baseando-se nas evidências científicas fornecidas no contexto.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"Contexto médico: {context}\\n\\nPergunta: {question}\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": answer\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "        \n",
        "        formatted.append(example)\n",
        "    \n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        for item in formatted:\n",
        "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "    \n",
        "    return len(formatted)\n",
        "\n",
        "os.makedirs('data_curated', exist_ok=True)\n",
        "\n",
        "print(\"Formatando para fine-tuning...\")\n",
        "\n",
        "with open('data_anonymized/test_set_anonymized.json', 'r', encoding='utf-8') as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "total = format_for_finetuning(test_data, 'data_curated/test_set_curated.jsonl')\n",
        "print(f\"Test set: {total} exemplos\")\n",
        "\n",
        "for i in range(10):\n",
        "    os.makedirs(f'data_curated/pqal_fold{i}', exist_ok=True)\n",
        "    \n",
        "    with open(f'data_anonymized/pqal_fold{i}/dev_set_anonymized.json', 'r', encoding='utf-8') as f:\n",
        "        fold_data = json.load(f)\n",
        "    \n",
        "    total = format_for_finetuning(fold_data, f'data_curated/pqal_fold{i}/dev_set_curated.jsonl')\n",
        "    print(f\"Fold {i}: {total} exemplos\")\n",
        "\n",
        "print(\"\\nFormatação concluída\")\n",
        "print(\"Formato: JSONL conversacional\")\n",
        "print(\"Estrutura: system + user + assistant\")\n",
        "print(\"Arquivos salvos em: data_curated/\")\n",
        "\n",
        "with open('data_curated/test_set_curated.jsonl', 'r', encoding='utf-8') as f:\n",
        "    example = json.loads(f.readline())\n",
        "    print(\"\\nExemplo:\")\n",
        "    print(json.dumps(example, indent=2, ensure_ascii=False)[:800] + \"...\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "aRFH2sIYarWe"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.12.10)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
