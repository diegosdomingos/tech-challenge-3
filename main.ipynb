{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Instala bibliotecas necess√°rias e importa m√≥dulos essenciais.\n",
        "\n"
      ],
      "metadata": {
        "id": "fdfFPEJL3CLJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rszAxEbZOA43"
      },
      "outputs": [],
      "source": [
        "!pip install -q unsloth[colab-new] faiss-cpu sentence-transformers trl transformers datasets\n",
        "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import faiss\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "from transformers import pipeline\n",
        "from transformers import TrainingArguments\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from datasets import load_dataset\n",
        "from datasets import Dataset\n",
        "from trl import SFTTrainer\n",
        "from dotenv import load_dotenv\n",
        "from huggingface_hub import login\n",
        "from datetime import date, timedelta"
      ],
      "metadata": {
        "id": "X1lx49AcLtBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7E3NuKW6Lyjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WyGzkc5ez3a"
      },
      "source": [
        "# Download e explora√ß√£o inicial dos dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "CuxJtGggAxGf"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/pubmedqa/pubmedqa.git\n",
        "\n",
        "file_path = 'pubmedqa/data/ori_pqal.json'\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "sample_key = list(data.keys())[0]\n",
        "print(f\"\\nCampos dispon√≠veis: {list(data[sample_key].keys())}\\n\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Explora√ß√£o de dados - PubMedQA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, key in enumerate(list(data.keys())[:3]):\n",
        "    item = data[key]\n",
        "\n",
        "    print(f\"\\nExemplo {i+1} | ID: {key}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"Question: {item.get('QUESTION', 'N/A')}\")\n",
        "\n",
        "    context = \" \".join(item.get('CONTEXTS', []))\n",
        "    print(f\"Context: {context[:300]}...\")\n",
        "\n",
        "    print(f\"Labels: {item.get('LABELS', 'N/A')}\")\n",
        "    print(f\"Decision: {item.get('final_decision', 'N/A')}\")\n",
        "    print(f\"Answer: {item.get('LONG_ANSWER', 'N/A')[:200]}...\")\n",
        "    print(f\"Meshes: {item.get('MESHES', 'N/A')}\")\n",
        "    print(f\"Year: {item.get('YEAR', 'N/A')}\")\n",
        "    print(f\"Reasoning required pred: {item.get('reasoning_required_pred', 'N/A')}\")\n",
        "    print(f\"Reasoning free pred: {item.get('reasoning_free_pred', 'N/A')}\")\n",
        "\n",
        "print(f\"\\n\\nTotal de registros: {len(data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRFH2sIYarWe"
      },
      "source": [
        "# Pr√©-processamento e Prepara√ß√£o para RAG\n",
        "## - Limpeza, Normaliza√ß√£o e Anonimiza√ß√£o dos Textos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4fd06e94"
      },
      "outputs": [],
      "source": [
        "# Define a fun√ß√£o para anonimizar dados sens√≠veis em um texto.\n",
        "def anonymize_text(text):\n",
        "    \"\"\"Remove dados sens√≠veis (LGPD/HIPAA compliance)\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = re.sub(r'(Dr\\.|Dra\\.|Doctor|Prof\\.|MD)\\s+[A-Z][a-z]+(\\s+[A-Z][a-z]+)?', '[NOME]', text)\n",
        "    text = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', '[EMAIL]', text)\n",
        "    locations = r'(Israel|Denmark|Chile|Texas|France|United Kingdom|UK|USA|Pakistan|Karachi|Jordan|Japan|Australia|North Carolina|Washington)'\n",
        "    text = re.sub(locations, '[LOCAL]', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\b\\d{6,}\\b', '[ID]', text)\n",
        "    text = re.sub(r'\\b(19|20)\\d{2}\\b', '[ANO]', text)\n",
        "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '[URL]', text)\n",
        "    return text\n",
        "\n",
        "# Define a fun√ß√£o para limpar e normalizar o texto, aplicando tamb√©m a anonimiza√ß√£o.\n",
        "def clean_text(text):\n",
        "  if not text:\n",
        "    return \"\"\n",
        "  text = unicodedata.normalize(\"NFKC\", text)\n",
        "  text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "  text = anonymize_text(text)\n",
        "  return text\n",
        "\n",
        "rag_documents = []\n",
        "\n",
        "# Processa cada item dos dados, aplicando as fun√ß√µes de limpeza e anonimiza√ß√£o.\n",
        "# Cria uma lista de dicion√°rios com as informa√ß√µes processadas.\n",
        "for item in data.values():\n",
        "    question = clean_text(item.get(\"QUESTION\"))\n",
        "    context = clean_text(\" \".join(item.get(\"CONTEXTS\", [])))\n",
        "    answer = clean_text(item.get(\"LONG_ANSWER\", \"\"))\n",
        "\n",
        "    if not question or not context:\n",
        "        continue\n",
        "\n",
        "    text = f\"\"\"\n",
        "    Pergunta cient√≠fica:\n",
        "    {question}\n",
        "\n",
        "    Evid√™ncia:\n",
        "    {context}\n",
        "\n",
        "    Conclus√£o:\n",
        "    {answer}\n",
        "    \"\"\"\n",
        "\n",
        "    rag_documents.append(text.strip())\n",
        "\n",
        "# Exibe o n√∫mero total de documentos processados.\n",
        "print(len(rag_documents))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gera√ß√£o de Embeddings e Constru√ß√£o de √çndice FAISS"
      ],
      "metadata": {
        "id": "236_nIoK52PH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializa o modelo de embeddings para converter texto em vetores num√©ricos.\n",
        "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Gera os embeddings para todos os documentos RAG.\n",
        "embeddings = embedder.encode(rag_documents, show_progress_bar=True)\n",
        "\n",
        "# Cria um √≠ndice FAISS para busca eficiente de documentos similares.\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "\n",
        "# Adiciona os embeddings ao √≠ndice FAISS.\n",
        "index.add(np.array(embeddings))\n",
        "\n",
        "# Garante que o diret√≥rio para salvar os arquivos exista no Google Drive.\n",
        "os.makedirs('/content/drive/MyDrive/rag', exist_ok=True)\n",
        "\n",
        "# Salva o √≠ndice FAISS no Google Drive.\n",
        "faiss.write_index(index, \"/content/drive/MyDrive/rag/medical_index.faiss\")\n",
        "\n",
        "# Salva os documentos RAG originais em formato JSON no Google Drive.\n",
        "with open('/content/drive/MyDrive/rag/medical_docs.json', 'w') as f:\n",
        "  json.dump(rag_documents, f)"
      ],
      "metadata": {
        "id": "hm0TXEBX56wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cria√ß√£o de dataset de prontu√°rio (Fict√≠cio) com SQLite"
      ],
      "metadata": {
        "id": "Y-fFDBx0LMFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define o caminho para o arquivo do banco de dados SQLite.\n",
        "DB_PATH = \"prontuarios.db\"\n",
        "\n",
        "# Conecta ao banco de dados SQLite e cria um cursor.\n",
        "conn = sqlite3.connect(DB_PATH)\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Cria a tabela 'pacientes' se ela ainda n√£o existir, com as colunas especificadas.\n",
        "cursor.execute(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS pacientes (\n",
        "    patient_id TEXT PRIMARY KEY,\n",
        "    nome TEXT,\n",
        "    data_nascimento TEXT,\n",
        "    idade INTEGER,\n",
        "    sexo TEXT,\n",
        "    alergias TEXT,\n",
        "    comorbidades TEXT\n",
        ")\n",
        "\"\"\")\n",
        "\n",
        "# Cria a tabela 'atendimentos' se ela ainda n√£o existir, com as colunas especificadas e uma chave estrangeira para 'pacientes'.\n",
        "cursor.execute(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS atendimentos (\n",
        "    atendimento_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    patient_id TEXT,\n",
        "    data_atendimento TEXT,\n",
        "    queixa_principal TEXT,\n",
        "    anamnese TEXT,\n",
        "    diagnostico TEXT,\n",
        "    conduta TEXT,\n",
        "    tratamentos_em_andamento TEXT,\n",
        "    exames_solicitados TEXT,\n",
        "    observacoes TEXT,\n",
        "    FOREIGN KEY(patient_id) REFERENCES pacientes(patient_id)\n",
        ")\n",
        "\"\"\")\n",
        "\n",
        "# Salva as mudan√ßas no banco de dados e fecha a conex√£o.\n",
        "conn.commit()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "GZVUcyAWLQhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Populando os dados fict√≠cios"
      ],
      "metadata": {
        "id": "p5XrRqtaO2C0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Listas de dados fict√≠cios para nomes, diagn√≥sticos, alergias e comorbidades.\n",
        "nomes = [\n",
        "    \"Ana Paula Souza\", \"Ana Carolina Lima\", \"Bruno Silva\", \"Carlos Eduardo Rocha\",\n",
        "    \"Daniela Martins\", \"Eduardo Nogueira\", \"Fernanda Alves\", \"Gabriel Pacheco\",\n",
        "    \"Helena Ribeiro\", \"Igor Farias\", \"Juliana Torres\", \"Lucas Fernandes\",\n",
        "    \"Mariana Araujo\", \"Natalia Pacheco\", \"Otavio Nunes\", \"Paula Guedes\",\n",
        "    \"Rafael Moreira\", \"Sabrina Lopes\", \"Thiago Barros\", \"Vanessa Farias\",\n",
        "    \"William Teixeira\", \"Ana Beatriz Costa\"\n",
        "]\n",
        "\n",
        "diagnosticos = [\n",
        "    \"Hipertens√£o arterial sist√™mica\",\n",
        "    \"Diabetes mellitus tipo 2\",\n",
        "    \"Asma br√¥nquica\",\n",
        "    \"Infec√ß√£o do trato urin√°rio\",\n",
        "    \"Pneumonia adquirida na comunidade\",\n",
        "    \"Transtorno de ansiedade generalizada\",\n",
        "    \"Gastrite cr√¥nica\",\n",
        "    \"Enxaqueca cr√¥nica\"\n",
        "]\n",
        "\n",
        "alergias_lista = [\n",
        "    \"Dipirona\", \"Penicilina\", \"Sulfa\", \"Nenhuma conhecida\"\n",
        "]\n",
        "\n",
        "comorbidades_lista = [\n",
        "    \"Hipertens√£o\", \"Diabetes\", \"Dislipidemia\", \"Obesidade\", \"Nenhuma\"\n",
        "]\n",
        "\n",
        "# Fun√ß√£o auxiliar para gerar datas de nascimento baseadas na idade.\n",
        "def gerar_data_nascimento(idade):\n",
        "    hoje = date.today()\n",
        "    return hoje - timedelta(days=idade * 365)\n",
        "\n",
        "# Conecta ao banco de dados SQLite para inser√ß√£o dos dados.\n",
        "conn = sqlite3.connect(DB_PATH)\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Loop para gerar e inserir dados de pacientes fict√≠cios na tabela 'pacientes'.\n",
        "for i, nome in enumerate(nomes, start=1):\n",
        "    idade = random.randint(18, 85)\n",
        "    patient_id = f\"PAT{i:04d}\"\n",
        "\n",
        "    cursor.execute(\"\"\"\n",
        "        INSERT OR IGNORE INTO pacientes\n",
        "        VALUES (?, ?, ?, ?, ?, ?, ?)\n",
        "    \"\"\", (\n",
        "        patient_id,\n",
        "        nome,\n",
        "        gerar_data_nascimento(idade).isoformat(),\n",
        "        idade,\n",
        "        random.choice([\"F\", \"M\"]),\n",
        "        random.choice(alergias_lista),\n",
        "        random.choice(comorbidades_lista)\n",
        "    ))\n",
        "\n",
        "    # Loop interno para gerar e inserir m√∫ltiplos atendimentos para cada paciente na tabela 'atendimentos'.\n",
        "    for _ in range(random.randint(1, 4)):\n",
        "        cursor.execute(\"\"\"\n",
        "            INSERT INTO atendimentos (\n",
        "                patient_id,\n",
        "                data_atendimento,\n",
        "                queixa_principal,\n",
        "                anamnese,\n",
        "                diagnostico,\n",
        "                conduta,\n",
        "                tratamentos_em_andamento,\n",
        "                exames_solicitados,\n",
        "                observacoes\n",
        "            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "        \"\"\", (\n",
        "            patient_id,\n",
        "            (date.today() - timedelta(days=random.randint(1, 1200))).isoformat(),\n",
        "            \"Dor, mal-estar e sintomas gerais\",\n",
        "            \"Paciente relata in√≠cio dos sintomas h√° alguns dias, sem fatores agravantes claros.\",\n",
        "            random.choice(diagnosticos),\n",
        "            \"Conduta expectante e acompanhamento ambulatorial\",\n",
        "            \"Uso cont√≠nuo de medica√ß√£o conforme prescri√ß√£o\",\n",
        "            \"Hemograma completo, glicemia, PCR\",\n",
        "            \"Paciente orientado quanto aos sinais de alarme\"\n",
        "        ))\n",
        "\n",
        "# Salva as mudan√ßas no banco de dados e fecha a conex√£o.\n",
        "conn.commit()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "qiKMx2FwO039"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparando conjunto de dados em portugu√™s para treino de tradu√ß√£o"
      ],
      "metadata": {
        "id": "eOou-BxIaFMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clona o reposit√≥rio contendo o dataset para alinhamento de linguagem em portugu√™s.\n",
        "!git clone https://github.com/diegosdomingos/tech-challenge-3.git\n",
        "\n",
        "# Carrega o dataset a partir do arquivo JSONL.\n",
        "file_path = 'tech-challenge-3/data/language_alignment_pt.jsonl'\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files=\"/content/drive/MyDrive/rag/language_alignment_pt.jsonl\",  #Alterar quando o dataset j√° estiver na main\n",
        "    split=\"train\"\n",
        ")\n",
        "\n",
        "print(dataset.column_names)\n",
        "\n",
        "# Define uma fun√ß√£o para converter o formato de mensagens em texto para o treinamento do modelo.\n",
        "def messages_to_text(example):\n",
        "    text = \"\"\n",
        "    for msg in example[\"messages\"]:\n",
        "        if msg[\"role\"] == \"system\":\n",
        "            text += f\"<<SYS>>\\n{msg['content']}\\n<</SYS>>\\n\\n\"\n",
        "        elif msg[\"role\"] == \"user\":\n",
        "            text += f\"[INST] {msg['content']} [/INST]\\n\"\n",
        "        elif msg[\"role\"] == \"assistant\":\n",
        "            text += msg[\"content\"]\n",
        "    return {\"text\": text}\n",
        "\n",
        "# Aplica a fun√ß√£o de convers√£o ao dataset.\n",
        "dataset = dataset.map(\n",
        "    messages_to_text,\n",
        "    batched=False,\n",
        "    remove_columns=[\"messages\"]\n",
        ")\n",
        "\n",
        "print(dataset.column_names)\n",
        "print(dataset[0])"
      ],
      "metadata": {
        "id": "Q3r9Q4rFGh0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configura√ß√£o do Modelo para Treinamento com LoRA\n",
        "## - Carrega modelo base e aplica adapta√ß√£o LoRA para reduzir custo de treino."
      ],
      "metadata": {
        "id": "l9uXJezHAtls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega um modelo de linguagem pr√©-treinado (`llama-3-8b-bnb-4bit`) com configura√ß√µes espec√≠ficas de otimiza√ß√£o.\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Aplica PEFT (Parameter-Efficient Fine-Tuning) com LoRA ao modelo para treinamento eficiente.\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0.05,\n",
        ")"
      ],
      "metadata": {
        "id": "vaTkGNFE8NhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Treinamento Supervisionado do Assistente M√©dico em Portugu√™s"
      ],
      "metadata": {
        "id": "-rgHPIbcaTwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configura os argumentos para o treinamento do modelo, como diret√≥rio de sa√≠da, n√∫mero de √©pocas, tamanho do lote e taxa de aprendizado.\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=5,\n",
        "    save_strategy=\"epoch\",\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Inicializa o SFTTrainer (Supervised Fine-tuning Trainer) com o modelo, tokenizer e dataset de treinamento.\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset, #DataSet com dados Em Portugu√™s\n",
        "    args=training_args,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=512,\n",
        "    packing=False\n",
        ")\n",
        "\n",
        "# Inicia o processo de treinamento do modelo.\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "sefogQu3LnHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autentica√ß√£o e Upload do Modelo Treinado no HuggingFace"
      ],
      "metadata": {
        "id": "mWe9atPtaZ8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega vari√°veis de ambiente (como o token do Hugging Face) de um arquivo .env.\n",
        "# Opicional: Subir modelo no Hugging Face\n",
        "ENV_PATH = \"/content/drive/MyDrive/token-hf/env\"\n",
        "load_dotenv(ENV_PATH)\n",
        "\n",
        "# Realiza o login no Hugging Face Hub usando o token.\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "login(token=HF_TOKEN)\n",
        "\n",
        "# Define o nome do reposit√≥rio no Hugging Face para o upload do modelo.\n",
        "HF_REPO = f\"{os.getenv(\"HF_USER_REPO\")}/assistente-medico-lora\"\n",
        "\n",
        "# Realiza o upload do modelo treinado para o Hugging Face Hub.\n",
        "model.push_to_hub(HF_REPO)\n",
        "# Realiza o upload do tokenizer para o Hugging Face Hub.\n",
        "tokenizer.push_to_hub(HF_REPO)"
      ],
      "metadata": {
        "id": "8WSXHwDBXptL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PROMPTS utilizados no modelo"
      ],
      "metadata": {
        "id": "0o-ZBZ2nOLW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define o prompt do sistema para o assistente m√©dico virtual, estabelecendo seu papel, regras e tom de voz.\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Voc√™ √© um assistente m√©dico virtual.\n",
        "Responda sempre em portugu√™s, com linguagem clara, emp√°tica e baseada em evid√™ncias cient√≠ficas.\n",
        "\n",
        "Regras:\n",
        "- N√£o invente informa√ß√µes.\n",
        "- Se n√£o houver evid√™ncia suficiente, diga isso explicitamente.\n",
        "- N√£o prescreva rem√©dios ou medicamentos, e nem indique tratamentos espec√≠ficos.\n",
        "- Quando perguntarem por algum rem√©dio, voc√™ deve responder: N√£o estou autorizado a prescrever medicamentos, por favor, consulte um m√©dico. <FIM>\n",
        "- Sempre cite a fonte da informa√ß√£o cient√≠fica\n",
        "\n",
        "Importante:\n",
        "- Responda de forma resumida e objetiva e finalize sempre a primeira resposta objetiva com o texto: <FIM>\n",
        "\"\"\"\n",
        "\n",
        "# Define o prompt para classificar a mensagem do usu√°rio em categorias como inv√°lida, indevida, prontu√°rio, etc.\n",
        "CLASSIFICADOR_PROMPT = \"\"\"\n",
        "Analise a mensagem do usu√°rio e retorne APENAS um JSON v√°lido.\n",
        "\n",
        "Classifica√ß√µes poss√≠veis:\n",
        "- INVALIDA\n",
        "- INDEVIDA\n",
        "- PRECISA_MAIS_INFO\n",
        "- PRONTUARIO\n",
        "- QA\n",
        "\n",
        "Regras:\n",
        "1. PRONTUARIO ‚Üí consulta sobre dados cl√≠nicos de um paciente espec√≠fico. Obrigat√≥riamente deve receber o nome do paciente.\n",
        "\n",
        "2. PRECISA_MAIS_INFO ‚Üí consulta sobre dados cl√≠nicos de um paciente espec√≠fico, por√©m sem informar dados que possam identificar esse paciente (Nome, por exemplo)\n",
        "Exemplo: consultar prontu√°rio, trazer hist√≥rico de pacientes, consultar caso de febre, ou seja, qualquer solicita√ß√£o sem especificar o paciente alvo.\n",
        "\n",
        "3. QA ‚Üí pergunta cient√≠fica ou t√©cnica geral\n",
        "Exemplo: Qual √© o meio de transmiss√£o da febre amarela?, COVID √© transmiss√≠vel mesmo com m√°scara?, A vacina da gripe √© 100% eficaz para preven√ß√£o de infec√£o?\n",
        "\n",
        "4. INVALIDA ‚Üí textos vagos, sem sentido ou que n√£o seja poss√≠vel interpretar sem mais informa√ß√µes.\n",
        "Exemplo: palavras soltas, palavras desconhecidas, frases sem sentido, idiomas desconhecidos.\n",
        "\n",
        "5. INDEVIDA -> Solicita√ß√µes de recomenda√ß√£o direta de rem√©dio, medicamentos ou tratamentos.\n",
        "Exemplo: Qual o melhor rem√©dio para inflama√ß√£o?, Quantos gramas devo tomar de dipirona?, Indique um bom rem√©dio para dor de dentes?, O que devo tomar para enxaqueca?\n",
        "\n",
        "Formato EXATO da resposta (obrigat√≥rio):\n",
        "{{\"classificacao\": \"<UMA_DAS_OPCOES>\"}} \"<FIM>\"\n",
        "\n",
        "Mensagem do usu√°rio:\n",
        "\\\"\\\"\\\"{mensagem}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "# Define o prompt para extrair o nome do paciente de uma mensagem, com regras espec√≠ficas para identifica√ß√£o.\n",
        "EXTRATOR_NOME_PROMPT = \"\"\"\n",
        "Extraia o NOME do paciente mencionado na mensagem.\n",
        "\n",
        "Regras obrigat√≥rias:\n",
        "- Se houver nome E sobrenome, retorne o nome completo\n",
        "- Se houver APENAS um nome (ex: \"Ana\"), retorne esse nome\n",
        "- Se houver um √∫nico nome pr√≥prio comum em portugu√™s, retorne esse nome\n",
        "- N√£o invente sobrenomes\n",
        "- N√£o inclua aspas, colchetes ou marcadores de chat\n",
        "- Se n√£o houver nenhum nome identific√°vel, retorne: NAO_IDENTIFICADO\n",
        "\n",
        "Formato EXATO da resposta (Obrigat√≥rio):\n",
        "<nome_do_paciente>\n",
        "<FIM>\n",
        "\n",
        "Mensagem:\n",
        "\\\"\\\"\\\"{mensagem}\\\"\\\"\\\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "rPa7HrYxOPV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipelines utilizadas"
      ],
      "metadata": {
        "id": "erDDs0doOhv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configura o pipeline do LLM para consulta de artigos cient√≠ficos, com baixa temperatura para respostas determin√≠sticas.\n",
        "llm_consulta = pipeline(\n",
        "  \"text-generation\",\n",
        "  model=model,\n",
        "  tokenizer=tokenizer,\n",
        "  max_new_tokens=250,\n",
        "  temperature=0.0,\n",
        "  do_sample=False,\n",
        "  repetition_penalty=1.1,\n",
        "  return_full_text=False,\n",
        "  eos_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Configura o pipeline do LLM para intera√ß√µes de chat, com temperatura mais alta para respostas mais criativas e variadas.\n",
        "llm_chat = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=250,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Configura o pipeline do LLM para extra√ß√£o de nomes, com baixa temperatura para precis√£o na extra√ß√£o.\n",
        "llm_extracao = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=80,\n",
        "    do_sample=False,\n",
        "    temperature=0.0,\n",
        "    return_full_text=False\n",
        ")"
      ],
      "metadata": {
        "id": "Ft-83kqFOmxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Busca informa√ß√µes no Q.A. pubMedQA"
      ],
      "metadata": {
        "id": "vDpkA6Dla3rI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega os documentos m√©dicos e o √≠ndice FAISS salvos anteriormente.\n",
        "with open('/content/drive/MyDrive/rag/medical_docs.json') as f:\n",
        "  docs = json.load(f)\n",
        "\n",
        "index = faiss.read_index('/content/drive/MyDrive/rag/medical_index.faiss')\n",
        "\n",
        "# Define uma fun√ß√£o para recuperar contextos relevantes com base em uma pergunta, usando o √≠ndice FAISS.\n",
        "def retrieve_context(question, k=3):\n",
        "  q_emb = embedder.encode([question])\n",
        "  _, idx = index.search(q_emb, k)\n",
        "  return \"\\n\\n\".join([docs[i] for i in idx[0]])\n",
        "\n",
        "# Define a fun√ß√£o principal de chat m√©dico que formula um prompt com contexto e gera uma resposta usando o LLM de consulta.\n",
        "def query_QA(question):\n",
        "\n",
        "  # Adiciona o token de fim √† pergunta, se necess√°rio.\n",
        "  if \"<FIM>\" not in question:\n",
        "    question += \" <FIM>\"\n",
        "\n",
        "  # Recupera o contexto mais relevante para a pergunta.\n",
        "  context = retrieve_context(question)\n",
        "\n",
        "  # Constr√≥i o prompt para o LLM, incluindo o prompt do sistema, o contexto e a pergunta.\n",
        "  prompt = f\"\"\"\n",
        "{SYSTEM_PROMPT}\n",
        "\n",
        "\n",
        "Contexto cient√≠fico relevante:\n",
        "{context}\n",
        "\n",
        "\n",
        "Pergunta: {question}\n",
        "Resposta:\n",
        "\"\"\"\n",
        "  # Gera a resposta usando o LLM de consulta.\n",
        "  output = llm_consulta(prompt)[0][\"generated_text\"]\n",
        "\n",
        "  # Remove o token de fim e espa√ßos em branco da resposta gerada.\n",
        "  output = output.split(\"<FIM>\")[0]\n",
        "\n",
        "  return output.strip()"
      ],
      "metadata": {
        "id": "-rtcoC5nOohs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testes de Consulta ao Assistente M√©dico com Exemplos\n"
      ],
      "metadata": {
        "id": "mt3Jqcl4SMF2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06713958"
      },
      "source": [
        "# Exemplo de consulta normal ao assistente m√©dico.\n",
        "question = \"O que a literatura indica sobre o uso de aspirina em preven√ß√£o prim√°ria?\"\n",
        "print(f\"Resposta 1 (normal): {query_QA(question)}\")\n",
        "\n",
        "# Exemplo de consulta que testa a restri√ß√£o do assistente em prescrever medicamentos.\n",
        "question = \"Qual medicamento √© eficaz para pedra nos rins?\"\n",
        "print(f\"Resposta 2 (restri√ß√£o): {query_QA(question)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Montagem das consultas utilizados para extrair prontu√°rios do SQLite"
      ],
      "metadata": {
        "id": "FALsGqFcO8Op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fun√ß√£o para buscar pacientes no banco de dados por nome parcial, considerando diferentes padr√µes.\n",
        "def buscar_pacientes_por_nome(nome_parcial: str):\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    cursor = conn.cursor()\n",
        "    bind = nome_parcial.strip()\n",
        "\n",
        "    #print('[DEBUG BIND]', bind)\n",
        "\n",
        "    # Executa a consulta SQL para encontrar pacientes com nomes correspondentes.\n",
        "    cursor.execute(\"\"\"\n",
        "        SELECT patient_id, nome, idade\n",
        "        FROM pacientes\n",
        "        WHERE LOWER(nome) LIKE LOWER(?)\n",
        "        OR LOWER(nome) LIKE LOWER(? || ' %')\n",
        "        OR LOWER(nome) LIKE LOWER('% ' || ? || ' %')\n",
        "        OR LOWER(nome) LIKE LOWER('% ' || ?)\n",
        "    \"\"\", (bind, bind, bind, bind))\n",
        "\n",
        "    resultados = cursor.fetchall()\n",
        "    conn.close()\n",
        "    #print('[DEBUG RESULT]', resultados)\n",
        "    return resultados\n",
        "\n",
        "\n",
        "# Fun√ß√£o para buscar o prontu√°rio de um paciente espec√≠fico usando seu ID.\n",
        "def buscar_prontuario_por_patient_id(patient_id: int):\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Executa a consulta SQL para obter o diagn√≥stico e observa√ß√µes do atendimento.\n",
        "    cursor.execute(\"\"\"\n",
        "        SELECT diagnostico, observacoes\n",
        "        FROM atendimentos\n",
        "        WHERE patient_id = ?\n",
        "    \"\"\", (patient_id,))\n",
        "\n",
        "    prontuario = cursor.fetchone()\n",
        "    conn.close()\n",
        "    return prontuario"
      ],
      "metadata": {
        "id": "ToC5i_h-TwaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Controles para o chat do assistente\n"
      ],
      "metadata": {
        "id": "b449ih0OGNia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## - Extrai o nome do paciente da mensagem"
      ],
      "metadata": {
        "id": "RPbeqIQRYYzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fun√ß√£o para extrair o nome do paciente de uma mensagem do usu√°rio, utilizando um LLM espec√≠fico.\n",
        "def extrair_nome_paciente(mensagem: str) -> str | None:\n",
        "    system_prompt = (\n",
        "        \"Voc√™ extrai nomes de texto com alta precis√£o.\"\n",
        "        )\n",
        "\n",
        "    user_prompt = EXTRATOR_NOME_PROMPT.format(mensagem=mensagem)\n",
        "\n",
        "    prompt = formatar_chat_llama(system_prompt, user_prompt)\n",
        "\n",
        "    output = llm_extracao(\n",
        "        prompt,\n",
        "        max_new_tokens=80,\n",
        "        do_sample=False,\n",
        "        temperature=0.0,\n",
        "        return_full_text=False\n",
        "    )\n",
        "\n",
        "    if not output or not output[0][\"generated_text\"]:\n",
        "        return None\n",
        "\n",
        "    nome = output[0][\"generated_text\"].strip()\n",
        "\n",
        "    if \"<FIM>\" not in nome:\n",
        "      return None\n",
        "\n",
        "    if \"<FIM>\" in nome:\n",
        "      nome = nome.split(\"<FIM>\")[0].strip()\n",
        "\n",
        "    if nome == \"NAO_IDENTIFICADO\" or not nome:\n",
        "        return None\n",
        "\n",
        "    return nome"
      ],
      "metadata": {
        "id": "sjiGzI1qYTbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## - Cria um pequeno controle de estado para o CHAT"
      ],
      "metadata": {
        "id": "OPcFvwduYkmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializa o estado do chat para controlar o modo (ex: prontu√°rio) e dados parciais.\n",
        "estado_chat = {\n",
        "    \"modo\": None,            # None | \"PRONTUARIO\"\n",
        "    \"dados_parciais\": {}     # ex: {\"nome_paciente\": \"...\"}\n",
        "}\n",
        "\n",
        "# Fun√ß√£o para limpar o estado do chat, resetando o modo e os dados parciais.\n",
        "def limpar_estado():\n",
        "    estado_chat[\"modo\"] = None\n",
        "    estado_chat[\"dados_parciais\"].clear()"
      ],
      "metadata": {
        "id": "qcwCqbAqYpxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## - Formata o CHAT de acordo com o que o Llama espera"
      ],
      "metadata": {
        "id": "_Zp76elyYtWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fun√ß√£o para formatar prompts no estilo Llama para intera√ß√£o com o modelo.\n",
        "def formatar_chat_llama(system_prompt: str, user_prompt: str) -> str:\n",
        "    return f\"\"\"<|begin_of_text|>\n",
        "<|system|>\n",
        "{system_prompt}\n",
        "<|user|>\n",
        "{user_prompt}\n",
        "<|assistant|>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "p1s0HIyDY5iW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## - Chama a consulta cient√≠fica QA (RAG)"
      ],
      "metadata": {
        "id": "pFkI8sX0Y_S7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fun√ß√£o para responder a perguntas cient√≠ficas gerais utilizando o assistente m√©dico.\n",
        "def responder_qa(mensagem):\n",
        "    resposta = query_QA(mensagem)\n",
        "\n",
        "    if not resposta:\n",
        "        return (\n",
        "            \"N√£o encontrei uma resposta cient√≠fica clara no material dispon√≠vel üìö\\n\\n\"\n",
        "            \"Se quiser, pode reformular a pergunta.\"\n",
        "        )\n",
        "\n",
        "    return resposta"
      ],
      "metadata": {
        "id": "eClLoxhNZG4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## - Chama a consulta ao prontu√°rio (SQLite)"
      ],
      "metadata": {
        "id": "6YRu8tNCZLxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fun√ß√£o para responder a consultas de prontu√°rio, buscando informa√ß√µes do paciente no banco de dados.\n",
        "def responder_prontuario_com_nome(nome: str):\n",
        "    pacientes = buscar_pacientes_por_nome(nome)\n",
        "\n",
        "    if not pacientes:\n",
        "        limpar_estado()\n",
        "        return f\"N√£o encontrei nenhum prontu√°rio para {nome}.\"\n",
        "\n",
        "    if len(pacientes) > 1:\n",
        "        lista = \"\\n\".join(\n",
        "            f\"- {p[1]} (idade: {p[2]})\"\n",
        "            for p in pacientes\n",
        "        )\n",
        "        return (\n",
        "            \"Encontrei mais de um paciente com esse nome \\n\\n\"\n",
        "            \"Por favor, confirme qual deles voc√™ deseja consultar:\\n\"\n",
        "            f\"{lista}\"\n",
        "        )\n",
        "\n",
        "    patient_id, nome_completo, idade = pacientes[0]\n",
        "    prontuario = buscar_prontuario_por_patient_id(patient_id)\n",
        "\n",
        "    limpar_estado()\n",
        "\n",
        "    diagnostico, observacoes = prontuario\n",
        "\n",
        "    return (\n",
        "        \"Prontu√°rio do paciente\\n\\n\"\n",
        "        f\"‚Ä¢ Nome: {nome_completo}\\n\"\n",
        "        f\"‚Ä¢ Idade: {idade}\\n\"\n",
        "        f\"‚Ä¢ Diagn√≥stico: {diagnostico}\\n\"\n",
        "        f\"‚Ä¢ Observa√ß√µes: {observacoes}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "V7YAgcTvZSLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## - Faz a an√°lise do contexto da mensagem e classifica em 5 op√ß√µes:\n",
        "\n",
        "- PRONTUARIO - Consulta ao banco de prontu√°rio (SQLite)\n",
        "- PRECISA_MAIS_INFO - √â uma consulta ao prontu√°rio, mas faltam dados para completar a a√ß√£o.\n",
        "- QA - D√∫vida cient√≠fica, deve consultar o QA (RAG)\n",
        "- INVALIDA - Textos vagos, sem sentido ou que n√£o seja poss√≠vel interpretar\n",
        "- INDEVIDA - Solicita√ß√£o indevida. Exemplo: Pedir que o chat receite um medicamento ou instru√ß√µes de como administr√°-lo."
      ],
      "metadata": {
        "id": "TtnF3lguZiXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fun√ß√£o auxiliar para extrair a classifica√ß√£o de uma resposta do LLM, tratando o token de fim e parsing JSON.\n",
        "def extrair_classificacao(resposta: str) -> str:\n",
        "    if \"<FIM>\" in resposta:\n",
        "      resposta = resposta.split(\"<FIM>\")[0].strip()\n",
        "\n",
        "    json_str = resposta.strip()\n",
        "\n",
        "    try:\n",
        "        return json.loads(json_str)[\"classificacao\"].upper()\n",
        "    except Exception:\n",
        "        return \"INVALIDA\"\n",
        "\n",
        "# Fun√ß√£o principal para classificar uma mensagem do usu√°rio utilizando um LLM e o prompt de classifica√ß√£o.\n",
        "def classificar_mensagem(mensagem: str) -> str:\n",
        "    system_prompt = (\n",
        "        \"Voc√™ √© um assistente respons√°vel apenas por classificar mensagens. \"\n",
        "        \"Siga estritamente o formato solicitado.\"\n",
        "    )\n",
        "\n",
        "    user_prompt = CLASSIFICADOR_PROMPT.format(mensagem=mensagem)\n",
        "\n",
        "    prompt = formatar_chat_llama(system_prompt, user_prompt)\n",
        "\n",
        "    output = llm_chat(\n",
        "        prompt,\n",
        "        max_new_tokens=50,\n",
        "        do_sample=False,\n",
        "        temperature=0.0,\n",
        "        return_full_text=False\n",
        "    )\n",
        "\n",
        "    resposta = output[0][\"generated_text\"].strip()\n",
        "\n",
        "    return extrair_classificacao(resposta)"
      ],
      "metadata": {
        "id": "Tm82VekMZgpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Respostas padr√µes para mensagens que n√£o geram consulta √† dados"
      ],
      "metadata": {
        "id": "8APCMFLMa2YE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retorna uma mensagem padr√£o quando a entrada do usu√°rio √© inv√°lida.\n",
        "def resposta_invalida():\n",
        "    return (\n",
        "        \"Tive dificuldade para entender sua mensagem \\n\\n\"\n",
        "        \"Voc√™ pode reformular ou explicar um pouco melhor o que precisa?\"\n",
        "    )\n",
        "\n",
        "# Retorna uma mensagem padr√£o quando falta informa√ß√£o para identificar um paciente.\n",
        "def resposta_pedir_mais_info():\n",
        "    return (\n",
        "        \"N√£o foi poss√≠vel identificar o paciente em quest√£o \\n\\n\"\n",
        "        \"Verifique se digitou o nome do paciente corretamente\"\n",
        "    )\n",
        "\n",
        "# Retorna uma mensagem padr√£o para solicita√ß√µes indevidas (ex: prescri√ß√£o de medicamentos).\n",
        "def resposta_indevida():\n",
        "    return (\n",
        "        \"N√£o tenho permiss√£o para responder este tipo de d√∫vida  \\n\\n\"\n",
        "        \"Somente um M√©dico est√° apto para receitar ou indicar medicamentos\"\n",
        "    )"
      ],
      "metadata": {
        "id": "fEuzTZ3N2fqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Router para decidir o que a llm deve consultar Q.A. cient√≠fico ou Prontu√°rio no SQLite"
      ],
      "metadata": {
        "id": "fhwcRbt8QxAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fun√ß√£o principal do roteador de chat, que direciona a mensagem do usu√°rio com base em sua classifica√ß√£o.\n",
        "def chat_router(mensagem: str):\n",
        "    global estado_chat\n",
        "\n",
        "    # Se o chat estiver no modo \"PRONTUARIO\", tenta extrair o nome do paciente.\n",
        "    if estado_chat[\"modo\"] == \"PRONTUARIO\":\n",
        "        nome = extrair_nome_paciente(mensagem)\n",
        "\n",
        "        if not nome:\n",
        "            return (\n",
        "                \"Ainda preciso do nome completo do paciente para continuar\"\n",
        "            )\n",
        "\n",
        "        limpar_estado()\n",
        "        return responder_prontuario_com_nome(nome)\n",
        "\n",
        "    # Classifica a mensagem do usu√°rio para determinar o tipo de intera√ß√£o.\n",
        "    classificacao = classificar_mensagem(mensagem)\n",
        "\n",
        "    print(\"Assunto: \",classificacao)\n",
        "\n",
        "    # Se a mensagem for sobre um prontu√°rio, tenta extrair o nome e responde.\n",
        "    if classificacao == \"PRONTUARIO\":\n",
        "\n",
        "        nome = extrair_nome_paciente(mensagem)\n",
        "\n",
        "        if nome:\n",
        "            return responder_prontuario_com_nome(nome)\n",
        "\n",
        "        # Se n√£o houver nome, solicita mais informa√ß√µes e muda o modo do chat para \"PRONTUARIO\".\n",
        "        limpar_estado()\n",
        "        estado_chat[\"modo\"] = \"PRONTUARIO\"\n",
        "        return (\n",
        "            \"Certo, vou consultar um prontu√°rio \\n\\n\"\n",
        "            \"Pode me informar o nome completo do paciente?\"\n",
        "        )\n",
        "\n",
        "    # Se a mensagem for uma pergunta de QA, responde utilizando o LLM de consulta.\n",
        "    if classificacao == \"QA\":\n",
        "        limpar_estado()\n",
        "        return responder_qa(mensagem)\n",
        "\n",
        "    # Se a mensagem precisar de mais informa√ß√µes, retorna a resposta padr√£o para tal.\n",
        "    if classificacao == \"PRECISA_MAIS_INFO\":\n",
        "        limpar_estado()\n",
        "        return resposta_pedir_mais_info()\n",
        "\n",
        "    # Se a mensagem for indevida, retorna a resposta padr√£o para tal.\n",
        "    if classificacao == \"INDEVIDA\":\n",
        "        limpar_estado()\n",
        "        return resposta_indevida()\n",
        "\n",
        "    # Para qualquer outra classifica√ß√£o, retorna uma resposta de mensagem inv√°lida.\n",
        "    return resposta_invalida()\n"
      ],
      "metadata": {
        "id": "eTnZMwM-Qp74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Teste do assistente com a simula√ß√£o de um CHAT real"
      ],
      "metadata": {
        "id": "XgSYh1jmQ6mK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicia a interface de chat do assistente m√©dico.\n",
        "print(\"Assistente M√©dico iniciado\")\n",
        "print(\"Digite 'sair' para encerrar\\n\")\n",
        "\n",
        "# Loop principal do chat, que continua at√© o usu√°rio digitar 'sair'.\n",
        "while True:\n",
        "    user_input = input(\"Voc√™: \").strip()\n",
        "\n",
        "    # Verifica se o usu√°rio deseja encerrar a sess√£o.\n",
        "    if user_input.lower() in [\"sair\", \"exit\", \"quit\"]:\n",
        "        print(\"Assistente: Sess√£o encerrada.\")\n",
        "        break\n",
        "\n",
        "    # Roteia a mensagem do usu√°rio para a fun√ß√£o de resposta apropriada.\n",
        "    resposta = chat_router(user_input)\n",
        "    print(f\"\\nAssistente: {resposta}\\n\")\n",
        "    print(\"\\n\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "o3v0HtQ3GSCL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "aRFH2sIYarWe"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}