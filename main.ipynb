{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Instala bibliotecas necess√°rias e importa m√≥dulos essenciais.\n",
        "\n"
      ],
      "metadata": {
        "id": "fdfFPEJL3CLJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rszAxEbZOA43"
      },
      "outputs": [],
      "source": [
        "!pip install -q unsloth[colab-new] faiss-cpu sentence-transformers trl datasets scikit-learn langgraph langchain\n",
        "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "!pip install -U spacy transformers accelerate\n",
        "!python -m spacy download pt_core_news_lg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import faiss\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "from transformers import pipeline\n",
        "from transformers import Trainer\n",
        "from transformers import TrainingArguments\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from datasets import load_dataset\n",
        "from datasets import Dataset\n",
        "from trl import SFTTrainer\n",
        "from dotenv import load_dotenv\n",
        "from huggingface_hub import login\n",
        "from datetime import date, timedelta\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import AutoTokenizer\n",
        "from typing import TypedDict, Optional, Dict\n",
        "from langgraph.graph import StateGraph, END"
      ],
      "metadata": {
        "id": "X1lx49AcLtBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7E3NuKW6Lyjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WyGzkc5ez3a"
      },
      "source": [
        "# Download e explora√ß√£o inicial dos dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "CuxJtGggAxGf"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/pubmedqa/pubmedqa.git\n",
        "\n",
        "file_path = 'pubmedqa/data/ori_pqal.json'\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "sample_key = list(data.keys())[0]\n",
        "print(f\"\\nCampos dispon√≠veis: {list(data[sample_key].keys())}\\n\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Explora√ß√£o de dados - PubMedQA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, key in enumerate(list(data.keys())[:3]):\n",
        "    item = data[key]\n",
        "\n",
        "    print(f\"\\nExemplo {i+1} | ID: {key}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"Question: {item.get('QUESTION', 'N/A')}\")\n",
        "\n",
        "    context = \" \".join(item.get('CONTEXTS', []))\n",
        "    print(f\"Context: {context[:300]}...\")\n",
        "\n",
        "    print(f\"Labels: {item.get('LABELS', 'N/A')}\")\n",
        "    print(f\"Decision: {item.get('final_decision', 'N/A')}\")\n",
        "    print(f\"Answer: {item.get('LONG_ANSWER', 'N/A')[:200]}...\")\n",
        "    print(f\"Meshes: {item.get('MESHES', 'N/A')}\")\n",
        "    print(f\"Year: {item.get('YEAR', 'N/A')}\")\n",
        "    print(f\"Reasoning required pred: {item.get('reasoning_required_pred', 'N/A')}\")\n",
        "    print(f\"Reasoning free pred: {item.get('reasoning_free_pred', 'N/A')}\")\n",
        "\n",
        "print(f\"\\n\\nTotal de registros: {len(data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRFH2sIYarWe"
      },
      "source": [
        "# Pr√©-processamento e Prepara√ß√£o para RAG\n",
        "## - Limpeza, Normaliza√ß√£o e Anonimiza√ß√£o dos Textos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4fd06e94"
      },
      "outputs": [],
      "source": [
        "# Define a fun√ß√£o para anonimizar dados sens√≠veis em um texto.\n",
        "def anonymize_text(text):\n",
        "    \"\"\"Remove dados sens√≠veis (LGPD/HIPAA compliance)\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = re.sub(r'(Dr\\.|Dra\\.|Doctor|Prof\\.|MD)\\s+[A-Z][a-z]+(\\s+[A-Z][a-z]+)?', '[NOME]', text)\n",
        "    text = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', '[EMAIL]', text)\n",
        "    locations = r'(Israel|Denmark|Chile|Texas|France|United Kingdom|UK|USA|Pakistan|Karachi|Jordan|Japan|Australia|North Carolina|Washington)'\n",
        "    text = re.sub(locations, '[LOCAL]', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\b\\d{6,}\\b', '[ID]', text)\n",
        "    text = re.sub(r'\\b(19|20)\\d{2}\\b', '[ANO]', text)\n",
        "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '[URL]', text)\n",
        "    return text\n",
        "\n",
        "# Define a fun√ß√£o para limpar e normalizar o texto, aplicando tamb√©m a anonimiza√ß√£o.\n",
        "def clean_text(text):\n",
        "  if not text:\n",
        "    return \"\"\n",
        "  text = unicodedata.normalize(\"NFKC\", text)\n",
        "  text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "  text = anonymize_text(text)\n",
        "  return text\n",
        "\n",
        "rag_documents = []\n",
        "\n",
        "# Processa cada item dos dados, aplicando as fun√ß√µes de limpeza e anonimiza√ß√£o.\n",
        "# Cria uma lista de dicion√°rios com as informa√ß√µes processadas.\n",
        "for item in data.values():\n",
        "    question = clean_text(item.get(\"QUESTION\"))\n",
        "    context = clean_text(\" \".join(item.get(\"CONTEXTS\", [])))\n",
        "    answer = clean_text(item.get(\"LONG_ANSWER\", \"\"))\n",
        "\n",
        "    if not question or not context:\n",
        "        continue\n",
        "\n",
        "    text = f\"\"\"\n",
        "    Pergunta cient√≠fica:\n",
        "    {question}\n",
        "\n",
        "    Evid√™ncia:\n",
        "    {context}\n",
        "\n",
        "    Conclus√£o:\n",
        "    {answer}\n",
        "    \"\"\"\n",
        "\n",
        "    rag_documents.append(text.strip())\n",
        "\n",
        "# Exibe o n√∫mero total de documentos processados.\n",
        "print(len(rag_documents))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gera√ß√£o de Embeddings e Constru√ß√£o de √çndice FAISS"
      ],
      "metadata": {
        "id": "236_nIoK52PH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializa o modelo de embeddings para converter texto em vetores num√©ricos.\n",
        "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Gera os embeddings para todos os documentos RAG.\n",
        "embeddings = embedder.encode(rag_documents, show_progress_bar=True)\n",
        "\n",
        "# Cria um √≠ndice FAISS para busca eficiente de documentos similares.\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "\n",
        "# Adiciona os embeddings ao √≠ndice FAISS.\n",
        "index.add(np.array(embeddings))\n",
        "\n",
        "# Garante que o diret√≥rio para salvar os arquivos exista no Google Drive.\n",
        "os.makedirs('/content/drive/MyDrive/rag', exist_ok=True)\n",
        "\n",
        "# Salva o √≠ndice FAISS no Google Drive.\n",
        "faiss.write_index(index, \"/content/drive/MyDrive/rag/medical_index.faiss\")\n",
        "\n",
        "# Salva os documentos RAG originais em formato JSON no Google Drive.\n",
        "with open('/content/drive/MyDrive/rag/medical_docs.json', 'w') as f:\n",
        "  json.dump(rag_documents, f)"
      ],
      "metadata": {
        "id": "hm0TXEBX56wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cria√ß√£o de dataset de prontu√°rio (Fict√≠cio) com SQLite"
      ],
      "metadata": {
        "id": "Y-fFDBx0LMFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define o caminho para o arquivo do banco de dados SQLite.\n",
        "DB_PATH = \"prontuarios.db\"\n",
        "\n",
        "# Conecta ao banco de dados SQLite e cria um cursor.\n",
        "conn = sqlite3.connect(DB_PATH)\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Cria a tabela 'pacientes' se ela ainda n√£o existir, com as colunas especificadas.\n",
        "cursor.execute(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS pacientes (\n",
        "    patient_id TEXT PRIMARY KEY,\n",
        "    nome TEXT,\n",
        "    data_nascimento TEXT,\n",
        "    idade INTEGER,\n",
        "    sexo TEXT,\n",
        "    alergias TEXT,\n",
        "    comorbidades TEXT\n",
        ")\n",
        "\"\"\")\n",
        "\n",
        "# Cria a tabela 'atendimentos' se ela ainda n√£o existir, com as colunas especificadas e uma chave estrangeira para 'pacientes'.\n",
        "cursor.execute(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS atendimentos (\n",
        "    atendimento_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    patient_id TEXT,\n",
        "    data_atendimento TEXT,\n",
        "    queixa_principal TEXT,\n",
        "    anamnese TEXT,\n",
        "    diagnostico TEXT,\n",
        "    conduta TEXT,\n",
        "    tratamentos_em_andamento TEXT,\n",
        "    exames_solicitados TEXT,\n",
        "    observacoes TEXT,\n",
        "    FOREIGN KEY(patient_id) REFERENCES pacientes(patient_id)\n",
        ")\n",
        "\"\"\")\n",
        "\n",
        "# Salva as mudan√ßas no banco de dados e fecha a conex√£o.\n",
        "conn.commit()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "GZVUcyAWLQhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Populando os dados fict√≠cios"
      ],
      "metadata": {
        "id": "p5XrRqtaO2C0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Listas de dados fict√≠cios para nomes, diagn√≥sticos, alergias e comorbidades.\n",
        "nomes = [\n",
        "    \"Ana Paula Souza\", \"Ana Carolina Lima\", \"Bruno Silva\", \"Carlos Eduardo Rocha\",\n",
        "    \"Daniela Martins\", \"Eduardo Nogueira\", \"Fernanda Alves\", \"Gabriel Pacheco\",\n",
        "    \"Helena Ribeiro\", \"Igor Farias\", \"Juliana Torres\", \"Lucas Fernandes\",\n",
        "    \"Mariana Araujo\", \"Natalia Pacheco\", \"Otavio Nunes\", \"Paula Guedes\",\n",
        "    \"Rafael Moreira\", \"Sabrina Lopes\", \"Thiago Barros\", \"Vanessa Farias\",\n",
        "    \"William Teixeira\", \"Ana Beatriz Costa\"\n",
        "]\n",
        "\n",
        "diagnosticos = [\n",
        "    \"Hipertens√£o arterial sist√™mica\",\n",
        "    \"Diabetes mellitus tipo 2\",\n",
        "    \"Asma br√¥nquica\",\n",
        "    \"Infec√ß√£o do trato urin√°rio\",\n",
        "    \"Pneumonia adquirida na comunidade\",\n",
        "    \"Transtorno de ansiedade generalizada\",\n",
        "    \"Gastrite cr√¥nica\",\n",
        "    \"Enxaqueca cr√¥nica\"\n",
        "]\n",
        "\n",
        "alergias_lista = [\n",
        "    \"Dipirona\", \"Penicilina\", \"Sulfa\", \"Nenhuma conhecida\"\n",
        "]\n",
        "\n",
        "comorbidades_lista = [\n",
        "    \"Hipertens√£o\", \"Diabetes\", \"Dislipidemia\", \"Obesidade\", \"Nenhuma\"\n",
        "]\n",
        "\n",
        "# Fun√ß√£o auxiliar para gerar datas de nascimento baseadas na idade.\n",
        "def gerar_data_nascimento(idade):\n",
        "    hoje = date.today()\n",
        "    return hoje - timedelta(days=idade * 365)\n",
        "\n",
        "# Conecta ao banco de dados SQLite para inser√ß√£o dos dados.\n",
        "conn = sqlite3.connect(DB_PATH)\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Loop para gerar e inserir dados de pacientes fict√≠cios na tabela 'pacientes'.\n",
        "for i, nome in enumerate(nomes, start=1):\n",
        "    idade = random.randint(18, 85)\n",
        "    patient_id = f\"PAT{i:04d}\"\n",
        "\n",
        "    cursor.execute(\"\"\"\n",
        "        INSERT OR IGNORE INTO pacientes\n",
        "        VALUES (?, ?, ?, ?, ?, ?, ?)\n",
        "    \"\"\", (\n",
        "        patient_id,\n",
        "        nome,\n",
        "        gerar_data_nascimento(idade).isoformat(),\n",
        "        idade,\n",
        "        random.choice([\"F\", \"M\"]),\n",
        "        random.choice(alergias_lista),\n",
        "        random.choice(comorbidades_lista)\n",
        "    ))\n",
        "\n",
        "    # Loop interno para gerar e inserir m√∫ltiplos atendimentos para cada paciente na tabela 'atendimentos'.\n",
        "    for _ in range(random.randint(1, 4)):\n",
        "        cursor.execute(\"\"\"\n",
        "            INSERT INTO atendimentos (\n",
        "                patient_id,\n",
        "                data_atendimento,\n",
        "                queixa_principal,\n",
        "                anamnese,\n",
        "                diagnostico,\n",
        "                conduta,\n",
        "                tratamentos_em_andamento,\n",
        "                exames_solicitados,\n",
        "                observacoes\n",
        "            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "        \"\"\", (\n",
        "            patient_id,\n",
        "            (date.today() - timedelta(days=random.randint(1, 1200))).isoformat(),\n",
        "            \"Dor, mal-estar e sintomas gerais\",\n",
        "            \"Paciente relata in√≠cio dos sintomas h√° alguns dias, sem fatores agravantes claros.\",\n",
        "            random.choice(diagnosticos),\n",
        "            \"Conduta expectante e acompanhamento ambulatorial\",\n",
        "            \"Uso cont√≠nuo de medica√ß√£o conforme prescri√ß√£o\",\n",
        "            \"Hemograma completo, glicemia, PCR\",\n",
        "            \"Paciente orientado quanto aos sinais de alarme\"\n",
        "        ))\n",
        "\n",
        "# Salva as mudan√ßas no banco de dados e fecha a conex√£o.\n",
        "conn.commit()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "qiKMx2FwO039"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparando conjunto de dados em portugu√™s para treino de tradu√ß√£o"
      ],
      "metadata": {
        "id": "eOou-BxIaFMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clona o reposit√≥rio contendo o dataset para alinhamento de linguagem em portugu√™s.\n",
        "!git clone https://github.com/diegosdomingos/tech-challenge-3.git\n",
        "\n",
        "# Carrega o dataset a partir do arquivo JSONL.\n",
        "file_path = 'tech-challenge-3/data/language_alignment_pt.jsonl'\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files=\"/content/drive/MyDrive/rag/language_alignment_pt.jsonl\",  #Alterar quando o dataset j√° estiver na main\n",
        "    split=\"train\"\n",
        ")\n",
        "\n",
        "print(dataset.column_names)\n",
        "\n",
        "# Define uma fun√ß√£o para converter o formato de mensagens em texto para o treinamento do modelo.\n",
        "def messages_to_text(example):\n",
        "    text = \"\"\n",
        "    for msg in example[\"messages\"]:\n",
        "        if msg[\"role\"] == \"system\":\n",
        "            text += f\"<<SYS>>\\n{msg['content']}\\n<</SYS>>\\n\\n\"\n",
        "        elif msg[\"role\"] == \"user\":\n",
        "            text += f\"[INST] {msg['content']} [/INST]\\n\"\n",
        "        elif msg[\"role\"] == \"assistant\":\n",
        "            text += msg[\"content\"]\n",
        "    return {\"text\": text}\n",
        "\n",
        "# Aplica a fun√ß√£o de convers√£o ao dataset.\n",
        "dataset = dataset.map(\n",
        "    messages_to_text,\n",
        "    batched=False,\n",
        "    remove_columns=[\"messages\"]\n",
        ")\n",
        "\n",
        "print(dataset.column_names)\n",
        "print(dataset[0])"
      ],
      "metadata": {
        "id": "Q3r9Q4rFGh0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configura√ß√£o do Modelo para Treinamento com LoRA\n",
        "## - Carrega modelo base e aplica adapta√ß√£o LoRA para reduzir custo de treino."
      ],
      "metadata": {
        "id": "l9uXJezHAtls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega um modelo de linguagem pr√©-treinado (`llama-3-8b-bnb-4bit`) com configura√ß√µes espec√≠ficas de otimiza√ß√£o.\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Aplica PEFT (Parameter-Efficient Fine-Tuning) com LoRA ao modelo para treinamento eficiente.\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0.05,\n",
        ")"
      ],
      "metadata": {
        "id": "vaTkGNFE8NhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Treinamento Supervisionado do Assistente M√©dico em Portugu√™s"
      ],
      "metadata": {
        "id": "-rgHPIbcaTwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configura os argumentos para o treinamento do modelo, como diret√≥rio de sa√≠da, n√∫mero de √©pocas, tamanho do lote e taxa de aprendizado.\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=4,\n",
        "    per_device_train_batch_size=4,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=5,\n",
        "    save_strategy=\"epoch\",\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Inicializa o SFTTrainer (Supervised Fine-tuning Trainer) com o modelo, tokenizer e dataset de treinamento.\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset, #DataSet com dados Em Portugu√™s\n",
        "    args=training_args,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=512,\n",
        "    packing=False\n",
        ")\n",
        "\n",
        "# Inicia o processo de treinamento do modelo.\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "sefogQu3LnHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autentica√ß√£o e Upload do Modelo Treinado no HuggingFace"
      ],
      "metadata": {
        "id": "mWe9atPtaZ8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega vari√°veis de ambiente (como o token do Hugging Face) de um arquivo .env.\n",
        "# Opicional: Subir modelo no Hugging Face\n",
        "ENV_PATH = \"/content/drive/MyDrive/token-hf/env\"\n",
        "load_dotenv(ENV_PATH)\n",
        "\n",
        "# Realiza o login no Hugging Face Hub usando o token.\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "login(token=HF_TOKEN)\n",
        "\n",
        "# Define o nome do reposit√≥rio no Hugging Face para o upload do modelo.\n",
        "HF_REPO = f\"{os.getenv(\"HF_USER_REPO\")}/assistente-medico-lora\"\n",
        "\n",
        "# Realiza o upload do modelo treinado para o Hugging Face Hub.\n",
        "model.push_to_hub(HF_REPO)\n",
        "# Realiza o upload do tokenizer para o Hugging Face Hub.\n",
        "tokenizer.push_to_hub(HF_REPO)"
      ],
      "metadata": {
        "id": "8WSXHwDBXptL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PROMPT utilizados no modelo"
      ],
      "metadata": {
        "id": "0o-ZBZ2nOLW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define o prompt do sistema para o assistente m√©dico virtual, estabelecendo seu papel, regras e tom de voz.\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Voc√™ √© um assistente m√©dico virtual.\n",
        "Responda sempre em portugu√™s, com linguagem clara, emp√°tica e baseada em evid√™ncias cient√≠ficas.\n",
        "\n",
        "Regras:\n",
        "- N√£o invente informa√ß√µes.\n",
        "- Se n√£o houver evid√™ncia suficiente, diga isso explicitamente.\n",
        "- N√£o prescreva rem√©dios ou medicamentos, e nem indique tratamentos espec√≠ficos.\n",
        "- Quando perguntarem por algum rem√©dio, voc√™ deve responder: N√£o estou autorizado a prescrever medicamentos, por favor, consulte um m√©dico. <FIM>\n",
        "- Sempre cite a fonte da informa√ß√£o cient√≠fica\n",
        "\n",
        "Importante:\n",
        "- Responda de forma resumida e objetiva e finalize sempre a primeira resposta objetiva com o texto: <FIM>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "rPa7HrYxOPV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline utilizadas"
      ],
      "metadata": {
        "id": "erDDs0doOhv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configura o pipeline do LLM para consulta de artigos cient√≠ficos, com baixa temperatura para respostas determin√≠sticas.\n",
        "llm_consulta = pipeline(\n",
        "  \"text-generation\",\n",
        "  model=model,\n",
        "  tokenizer=tokenizer,\n",
        "  max_new_tokens=250,\n",
        "  temperature=0.0,\n",
        "  do_sample=False,\n",
        "  repetition_penalty=1.1,\n",
        "  return_full_text=False,\n",
        "  eos_token_id=tokenizer.eos_token_id\n",
        ")"
      ],
      "metadata": {
        "id": "Ft-83kqFOmxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Busca informa√ß√µes no Q.A. pubMedQA"
      ],
      "metadata": {
        "id": "vDpkA6Dla3rI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega os documentos m√©dicos e o √≠ndice FAISS salvos anteriormente.\n",
        "with open('/content/drive/MyDrive/rag/medical_docs.json') as f:\n",
        "  docs = json.load(f)\n",
        "\n",
        "index = faiss.read_index('/content/drive/MyDrive/rag/medical_index.faiss')\n",
        "\n",
        "# Define uma fun√ß√£o para recuperar contextos relevantes com base em uma pergunta, usando o √≠ndice FAISS.\n",
        "def retrieve_context(question, k=3):\n",
        "  q_emb = embedder.encode([question])\n",
        "  _, idx = index.search(q_emb, k)\n",
        "  return \"\\n\\n\".join([docs[i] for i in idx[0]])\n",
        "\n",
        "# Define a fun√ß√£o principal de chat m√©dico que formula um prompt com contexto e gera uma resposta usando o LLM de consulta.\n",
        "def query_QA(question):\n",
        "\n",
        "  # Adiciona o token de fim √† pergunta, se necess√°rio.\n",
        "  if \"<FIM>\" not in question:\n",
        "    question += \" <FIM>\"\n",
        "\n",
        "  # Recupera o contexto mais relevante para a pergunta.\n",
        "  context = retrieve_context(question)\n",
        "\n",
        "  # Constr√≥i o prompt para o LLM, incluindo o prompt do sistema, o contexto e a pergunta.\n",
        "  prompt = f\"\"\"\n",
        "{SYSTEM_PROMPT}\n",
        "\n",
        "\n",
        "Contexto cient√≠fico relevante:\n",
        "{context}\n",
        "\n",
        "\n",
        "Pergunta: {question}\n",
        "Resposta:\n",
        "\"\"\"\n",
        "  # Gera a resposta usando o LLM de consulta.\n",
        "  output = llm_consulta(prompt)[0][\"generated_text\"]\n",
        "\n",
        "  # Remove o token de fim e espa√ßos em branco da resposta gerada.\n",
        "  output = output.split(\"<FIM>\")[0]\n",
        "\n",
        "  return output.strip()"
      ],
      "metadata": {
        "id": "-rtcoC5nOohs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testes de Consulta ao Assistente M√©dico com Exemplos\n"
      ],
      "metadata": {
        "id": "mt3Jqcl4SMF2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06713958"
      },
      "source": [
        "# Exemplo de consulta normal ao assistente m√©dico.\n",
        "question = \"O que a literatura indica sobre o uso de aspirina em preven√ß√£o prim√°ria?\"\n",
        "print(f\"Resposta 1 (normal): {query_QA(question)}\")\n",
        "\n",
        "# Exemplo de consulta que testa a restri√ß√£o do assistente em prescrever medicamentos.\n",
        "question = \"Qual medicamento √© eficaz para pedra nos rins?\"\n",
        "print(f\"Resposta 2 (restri√ß√£o): {query_QA(question)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Montagem das consultas utilizados para extrair prontu√°rios do SQLite"
      ],
      "metadata": {
        "id": "FALsGqFcO8Op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fun√ß√£o para buscar pacientes no banco de dados por nome parcial, considerando diferentes padr√µes.\n",
        "def buscar_pacientes_por_nome(nome_parcial: str):\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    cursor = conn.cursor()\n",
        "    bind = nome_parcial.strip()\n",
        "\n",
        "    #print('[DEBUG BIND]', bind)\n",
        "\n",
        "    # Executa a consulta SQL para encontrar pacientes com nomes correspondentes.\n",
        "    cursor.execute(\"\"\"\n",
        "        SELECT patient_id, nome, idade\n",
        "        FROM pacientes\n",
        "        WHERE LOWER(nome) LIKE LOWER(?)\n",
        "        OR LOWER(nome) LIKE LOWER(? || ' %')\n",
        "        OR LOWER(nome) LIKE LOWER('% ' || ? || ' %')\n",
        "        OR LOWER(nome) LIKE LOWER('% ' || ?)\n",
        "    \"\"\", (bind, bind, bind, bind))\n",
        "\n",
        "    resultados = cursor.fetchall()\n",
        "    conn.close()\n",
        "    #print('[DEBUG RESULT]', resultados)\n",
        "    return resultados\n",
        "\n",
        "\n",
        "# Fun√ß√£o para buscar o prontu√°rio de um paciente espec√≠fico usando seu ID.\n",
        "def buscar_prontuario_por_patient_id(patient_id: int):\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Executa a consulta SQL para obter o diagn√≥stico e observa√ß√µes do atendimento.\n",
        "    cursor.execute(\"\"\"\n",
        "        SELECT diagnostico, observacoes\n",
        "        FROM atendimentos\n",
        "        WHERE patient_id = ?\n",
        "    \"\"\", (patient_id,))\n",
        "\n",
        "    prontuario = cursor.fetchone()\n",
        "    conn.close()\n",
        "    return prontuario"
      ],
      "metadata": {
        "id": "ToC5i_h-TwaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Controles para o chat do assistente\n"
      ],
      "metadata": {
        "id": "b449ih0OGNia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## - Utiliza o Spacy para extrair o nome do paciente\n",
        "\n"
      ],
      "metadata": {
        "id": "RPbeqIQRYYzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extrai o nome do paciente do texto\n",
        "nlp = spacy.load(\"pt_core_news_lg\")\n",
        "\n",
        "def extrair_nome_paciente(texto: str):\n",
        "\n",
        "    texto = texto.split()\n",
        "    texto = \" \".join(p.capitalize() for p in texto)\n",
        "\n",
        "    doc = nlp(texto)\n",
        "    nomes = [ent.text for ent in doc.ents if ent.label_ == \"PER\"]\n",
        "\n",
        "    if not nomes:\n",
        "        return None\n",
        "\n",
        "    return nomes[0]\n"
      ],
      "metadata": {
        "id": "sjiGzI1qYTbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## - Chama a consulta cient√≠fica QA (RAG)"
      ],
      "metadata": {
        "id": "pFkI8sX0Y_S7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fun√ß√£o para responder a perguntas cient√≠ficas gerais utilizando o assistente m√©dico.\n",
        "def responder_qa(mensagem):\n",
        "    resposta = query_QA(mensagem)\n",
        "\n",
        "    if not resposta:\n",
        "        return (\n",
        "            \"N√£o encontrei uma resposta cient√≠fica clara no material dispon√≠vel üìö\\n\\n\"\n",
        "            \"Se quiser, pode reformular a pergunta.\"\n",
        "        )\n",
        "\n",
        "    return resposta"
      ],
      "metadata": {
        "id": "eClLoxhNZG4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## - Chama a consulta ao prontu√°rio (SQLite)"
      ],
      "metadata": {
        "id": "6YRu8tNCZLxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fun√ß√£o para responder a consultas de prontu√°rio, buscando informa√ß√µes do paciente no banco de dados.\n",
        "def responder_prontuario_com_nome(nome: str):\n",
        "    pacientes = buscar_pacientes_por_nome(nome)\n",
        "\n",
        "    if not pacientes:\n",
        "        return False, f\"N√£o encontrei nenhum prontu√°rio para {nome}.\"\n",
        "\n",
        "    if len(pacientes) > 1:\n",
        "        lista = \"\\n\".join(\n",
        "            f\"- {p[1]} (idade: {p[2]})\"\n",
        "            for p in pacientes\n",
        "        )\n",
        "        return (False,\n",
        "            \"Encontrei mais de um paciente com esse nome \\n\\n\"\n",
        "            \"Por favor, confirme qual deles voc√™ deseja consultar:\\n\"\n",
        "            f\"{lista}\"\n",
        "        )\n",
        "\n",
        "    patient_id, nome_completo, idade = pacientes[0]\n",
        "    prontuario = buscar_prontuario_por_patient_id(patient_id)\n",
        "\n",
        "    diagnostico, observacoes = prontuario\n",
        "\n",
        "    return (True,\n",
        "        \"Prontu√°rio do paciente\\n\\n\"\n",
        "        f\"‚Ä¢ Nome: {nome_completo}\\n\"\n",
        "        f\"‚Ä¢ Idade: {idade}\\n\"\n",
        "        f\"‚Ä¢ Diagn√≥stico: {diagnostico}\\n\"\n",
        "        f\"‚Ä¢ Observa√ß√µes: {observacoes}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "V7YAgcTvZSLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## - Fine-tunning de modelo para fazer a classifica√ß√£o da solicita√ß√£o do usu√°rio\n",
        "\n",
        "- PRONTUARIO - Consulta ao banco de prontu√°rio (SQLite)\n",
        "- PRECISA_MAIS_INFO - √â uma consulta ao prontu√°rio, mas faltam dados para completar a a√ß√£o.\n",
        "- QA - D√∫vida cient√≠fica, deve consultar o QA (RAG)\n",
        "- INVALIDA - Textos vagos, sem sentido ou que n√£o seja poss√≠vel interpretar\n",
        "- INDEVIDA - Solicita√ß√£o indevida. Exemplo: Pedir que o chat receite um medicamento ou instru√ß√µes de como administr√°-lo."
      ],
      "metadata": {
        "id": "fCUTl5V_igcY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## - Prepara o dataset"
      ],
      "metadata": {
        "id": "va8WIa8mqHFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"neuralmind/bert-base-portuguese-cased\"\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files=\"/content/drive/MyDrive/rag/dataset_intention.jsonl\"\n",
        ")\n",
        "\n",
        "dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=64\n",
        "    )\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
        "\n",
        "labels = [\"PRONTUARIO\", \"QA\", \"INDEVIDA\"]\n",
        "label2id = {l: i for i, l in enumerate(labels)}\n",
        "id2label = {i: l for l, i in label2id.items()}\n",
        "\n",
        "def encode_labels(batch):\n",
        "    batch[\"label\"] = label2id[batch[\"label\"]]\n",
        "    return batch\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.map(encode_labels)\n",
        "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
        "tokenized_dataset.set_format(\"torch\")\n",
        "\n",
        "model_classifier = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=len(labels),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")"
      ],
      "metadata": {
        "id": "c2qZOtEnqcdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## - Treina o modelo"
      ],
      "metadata": {
        "id": "p0HxRWQpqUfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return {\"accuracy\": accuracy_score(labels, preds)}\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./intent_model\",\n",
        "    do_eval=True,\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=4,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        "    logging_steps=20,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model_classifier, #Modelo classifier\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "i5Dm0fz6qZSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## - Prepara o pipeline"
      ],
      "metadata": {
        "id": "FQpcsptwqkk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "intent_classifier = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=model_classifier,\n",
        "    tokenizer=tokenizer,\n",
        "    return_all_scores=False\n",
        ")"
      ],
      "metadata": {
        "id": "5brOdWu9qqqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## - Fun√ß√£o de Classifica√ß√£o"
      ],
      "metadata": {
        "id": "_v-ddcDjc-hR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_ruido(texto: str) -> bool:\n",
        "    texto = texto.strip()\n",
        "\n",
        "    # Muito curto\n",
        "    if len(texto) < 4:\n",
        "        return True\n",
        "\n",
        "    # Sem vogais (ex: asdfghj)\n",
        "    if not re.search(r\"[aeiou√°√©√≠√≥√∫√£√µ]\", texto.lower()):\n",
        "        return True\n",
        "\n",
        "    # Muitos caracteres repetidos\n",
        "    if re.search(r\"(.)\\1{4,}\", texto):\n",
        "        return True\n",
        "\n",
        "    # S√≥ s√≠mbolos / n√∫meros\n",
        "    if re.fullmatch(r\"[^a-zA-Z√Ä-√ø]+\", texto):\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def classificar_intencao(texto: str):\n",
        "    texto_lower = texto.lower().strip()\n",
        "\n",
        "    if is_ruido(texto):\n",
        "        return \"INVALIDA\"\n",
        "\n",
        "    if len(texto_lower) < 4:\n",
        "        return \"INVALIDA\"\n",
        "\n",
        "    if any(p in texto_lower for p in [\n",
        "        \"prescreva\", \"prescrever\", \"prescri√ß√£o\", \"rem√©dio\", \"medica√ß√£o\"\n",
        "    ]):\n",
        "        return \"INDEVIDA\"\n",
        "\n",
        "    if \"prontu√°rio\" in texto_lower or \"prontuario\" in texto_lower:\n",
        "        return \"PRONTUARIO\"\n",
        "\n",
        "    try:\n",
        "        result = intent_classifier(texto)[0]\n",
        "        label = result.get(\"label\")\n",
        "\n",
        "        if label not in {\"PRONTUARIO\", \"QA\", \"INDEVIDA\"}:\n",
        "            return \"INVALIDA\"\n",
        "\n",
        "        return label\n",
        "    except Exception:\n",
        "        return \"INVALIDA\""
      ],
      "metadata": {
        "id": "3UpPAQ84dBQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## - Respostas padr√µes para mensagens que n√£o geram consulta √† dados"
      ],
      "metadata": {
        "id": "8APCMFLMa2YE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "RESPOSTAS_INVALIDA = [\n",
        "    \"N√£o consegui entender bem sua solicita√ß√£o. Voc√™ pode reformular ou explicar um pouco melhor o que precisa?\",\n",
        "    \"Sua mensagem n√£o ficou muito clara para mim. Pode tentar explicar de outra forma?\",\n",
        "    \"Tive dificuldade para compreender sua solicita√ß√£o. Voc√™ poderia fornecer mais detalhes?\",\n",
        "    \"N√£o entendi exatamente o que voc√™ deseja no momento. Pode esclarecer um pouco mais?\",\n",
        "    \"Posso ajudar com consultas a prontu√°rios espec√≠ficos ou com perguntas cient√≠ficas. Voc√™ pode reformular sua solicita√ß√£o?\",\n",
        "    \"Se voc√™ deseja consultar um prontu√°rio, informe o nome do paciente. Caso contr√°rio, explique melhor sua d√∫vida.\",\n",
        "    \"N√£o consegui entender sua solicita√ß√£o. Voc√™ pode informar se deseja consultar um prontu√°rio ou fazer uma pergunta cient√≠fica?\",\n",
        "    \"Para que eu possa ajudar, indique o nome do paciente ou detalhe melhor o tipo de informa√ß√£o que voc√™ procura.\"\n",
        "]\n",
        "\n",
        "# Retorna uma mensagem padr√£o quando a entrada do usu√°rio √© inv√°lida.\n",
        "def resposta_invalida():\n",
        "    return random.choice(RESPOSTAS_INVALIDA)\n",
        "\n",
        "\n",
        "# Retorna uma mensagem padr√£o para solicita√ß√µes indevidas (ex: prescri√ß√£o de medicamentos).\n",
        "def resposta_indevida():\n",
        "    return (\n",
        "        \"N√£o tenho permiss√£o para responder este tipo de d√∫vida  \\n\\n\"\n",
        "        \"Somente um M√©dico est√° apto para receitar ou indicar medicamentos\"\n",
        "    )\n",
        "\n",
        "def solicita_nome_paciente():\n",
        "    return (\n",
        "        \"Por favor, informe o nome completo do paciente.\"\n",
        "    )"
      ],
      "metadata": {
        "id": "fEuzTZ3N2fqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## - Defini√ß√£o do estado do grafo"
      ],
      "metadata": {
        "id": "6Gn6BCN0vRsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatState(TypedDict):\n",
        "    user_input: str\n",
        "    intent: Optional[str]\n",
        "    patient_name: Optional[str]\n",
        "    awaiting_patient_name: bool\n",
        "    response: Optional[str]"
      ],
      "metadata": {
        "id": "pa-Uo3K9vVHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## - Nodes utilizados\n",
        "\n"
      ],
      "metadata": {
        "id": "4qkE6ptYvZ4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MSG_PEDIR_NOME = solicita_nome_paciente()\n",
        "MSG_INDEVIDA = resposta_indevida()\n",
        "MSG_INVALIDA = resposta_invalida()\n",
        "\n",
        "def entry_router(state: ChatState) -> str:\n",
        "    if state.get(\"awaiting_patient_name\"):\n",
        "        print(\"Assunto:\",\"PRONTUARIO\")\n",
        "        return \"prontuario\"\n",
        "    return \"classify\"\n",
        "\n",
        "# Node Classifica√ß√£o\n",
        "def classify_intent_node(state: ChatState) -> ChatState:\n",
        "\n",
        "    text = state[\"user_input\"]\n",
        "\n",
        "    intent = classificar_intencao(text)\n",
        "\n",
        "    print(\"Assunto:\",intent)\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"intent\": intent\n",
        "    }\n",
        "\n",
        "# Node Prontu√°rio\n",
        "def prontuario_node(state: ChatState) -> ChatState:\n",
        "    nome = extrair_nome_paciente(state[\"user_input\"])\n",
        "\n",
        "    if not nome:\n",
        "        return {\n",
        "            **state,\n",
        "            \"awaiting_patient_name\": True,\n",
        "            \"response\": MSG_PEDIR_NOME\n",
        "        }\n",
        "\n",
        "    prontuario_ok, prontuario_texto = responder_prontuario_com_nome(nome)\n",
        "\n",
        "    awaiting_patient = True\n",
        "    if prontuario_ok:\n",
        "      awaiting_patient = False\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"patient_name\": nome,\n",
        "        \"awaiting_patient_name\": awaiting_patient,\n",
        "        \"response\": prontuario_texto\n",
        "    }\n",
        "\n",
        "# Node QA\n",
        "def qa_node(state: ChatState) -> ChatState:\n",
        "    pergunta = state[\"user_input\"]\n",
        "\n",
        "    resposta = query_QA(pergunta)  # SUA FUN√á√ÉO RAG\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"response\": resposta\n",
        "    }\n",
        "\n",
        "# Node respostas simples\n",
        "def indevida_node(state: ChatState) -> ChatState:\n",
        "    return {**state, \"response\": MSG_INDEVIDA}\n",
        "\n",
        "def invalida_node(state: ChatState) -> ChatState:\n",
        "    return {**state, \"response\": MSG_INVALIDA}\n"
      ],
      "metadata": {
        "id": "xgaNb2vRveYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## - Router condicional"
      ],
      "metadata": {
        "id": "FvN5b1C7v2rH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def route_by_intent(state: ChatState) -> str:\n",
        "    intent = state[\"intent\"]\n",
        "\n",
        "    if intent == \"PRONTUARIO\":\n",
        "        return \"prontuario\"\n",
        "    elif intent == \"QA\":\n",
        "        return \"qa\"\n",
        "    elif intent == \"INDEVIDA\":\n",
        "        return \"indevida\"\n",
        "    else:\n",
        "        return \"invalida\""
      ],
      "metadata": {
        "id": "8CNFXMQ0v4wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## - GRAFO"
      ],
      "metadata": {
        "id": "TfL31fdgwASM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "graph = StateGraph(ChatState)\n",
        "\n",
        "# Nodes\n",
        "graph.add_node(\"classify\", classify_intent_node)\n",
        "graph.add_node(\"prontuario\", prontuario_node)\n",
        "graph.add_node(\"qa\", qa_node)\n",
        "graph.add_node(\"indevida\", indevida_node)\n",
        "graph.add_node(\"invalida\", invalida_node)\n",
        "\n",
        "# ENTRY POINT din√¢mico\n",
        "graph.set_conditional_entry_point(\n",
        "    entry_router,\n",
        "    {\n",
        "        \"classify\": \"classify\",\n",
        "        \"prontuario\": \"prontuario\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Roteamento ap√≥s classifica√ß√£o\n",
        "graph.add_conditional_edges(\n",
        "    \"classify\",\n",
        "    route_by_intent,\n",
        "    {\n",
        "        \"prontuario\": \"prontuario\",\n",
        "        \"qa\": \"qa\",\n",
        "        \"indevida\": \"indevida\",\n",
        "        \"invalida\": \"invalida\",\n",
        "    }\n",
        ")\n",
        "\n",
        "# Fim\n",
        "graph.add_edge(\"prontuario\", END)\n",
        "graph.add_edge(\"qa\", END)\n",
        "graph.add_edge(\"indevida\", END)\n",
        "graph.add_edge(\"invalida\", END)\n",
        "\n",
        "app = graph.compile()"
      ],
      "metadata": {
        "id": "6HCy6QI9wCQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Teste do assistente com a simula√ß√£o de um CHAT real"
      ],
      "metadata": {
        "id": "XgSYh1jmQ6mK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state = {\n",
        "    \"user_input\": \"\",\n",
        "    \"intent\": None,\n",
        "    \"patient_name\": None,\n",
        "    \"awaiting_patient_name\": False,\n",
        "    \"response\": None\n",
        "}\n",
        "\n",
        "# Inicia a interface de chat do assistente m√©dico.\n",
        "print(\"Assistente M√©dico iniciado\")\n",
        "print(\"Digite 'sair' para encerrar\\n\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"Voc√™: \")\n",
        "\n",
        "     # Verifica se o usu√°rio deseja encerrar a sess√£o.\n",
        "    if user_input.lower() in [\"sair\", \"exit\", \"quit\"]:\n",
        "        print(\"Assistente: Sess√£o encerrada.\")\n",
        "        break\n",
        "\n",
        "    state[\"user_input\"] = user_input\n",
        "\n",
        "    state = app.invoke(state)\n",
        "\n",
        "    print(f\"\\nAssistente: {state[\"response\"]}\\n\")\n",
        "    print(\"\\n\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "hERWiOJIwKnf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "aRFH2sIYarWe"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}