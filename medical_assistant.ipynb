{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMzipvzeIDnnW6p5ogSeY7h"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PREPARAÃ‡ÃƒO DO AMBIENTE\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2--8SMYwlmWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \\\n",
        "  transformers \\\n",
        "  accelerate \\\n",
        "  bitsandbytes \\\n",
        "  langchain-core \\\n",
        "  langchain-community \\\n",
        "  langgraph \\\n",
        "  python-dotenv \\\n",
        "  torch\n",
        "\n",
        "import os\n",
        "import re\n",
        "import torch\n",
        "from typing import TypedDict, List\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "from google.colab import drive\n",
        "from huggingface_hub import login\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "HMMm0D-tTbeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CARREGANDO O MODELO LORA"
      ],
      "metadata": {
        "id": "Hfl3i-GHougi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ENV_PATH = \"/content/drive/MyDrive/token-hf/env\"\n",
        "load_dotenv(ENV_PATH)\n",
        "\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "login(token=HF_TOKEN)\n",
        "\n",
        "HF_USER_REPO = os.getenv(\"HF_USER_REPO\")\n",
        "HF_REPO = f\"{HF_USER_REPO}/assistente-medico-lora\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(HF_REPO)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    HF_REPO,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n"
      ],
      "metadata": {
        "id": "bkygdWmSodGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PIPELINE HUGGINGFACE + LANGCHAIN LLM"
      ],
      "metadata": {
        "id": "_Bcfuq1_o7eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hf_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=128,\n",
        "    do_sample=False,\n",
        "    temperature=0.3,\n",
        "    repetition_penalty=1.1,\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=hf_pipeline)"
      ],
      "metadata": {
        "id": "LgFzX1I4pATE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PREPARANDO O ASSISTENTE"
      ],
      "metadata": {
        "id": "xl5YGnBKpBb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "VocÃª Ã© um assistente mÃ©dico-cientÃ­fico. Responda exclusivamente em portuguÃªs.\n",
        "\n",
        "Responda EXCLUSIVAMENTE com base no contexto fornecido.\n",
        "NÃ£o utilize conhecimento externo.\n",
        "\n",
        "REGRAS OBRIGATÃ“RIAS:\n",
        "- A resposta DEVE conter APENAS duas linhas.\n",
        "- A primeira linha DEVE comeÃ§ar exatamente com:\n",
        "  \"DecisÃ£o: SIM\", \"DecisÃ£o: NÃƒO\" ou \"DecisÃ£o: TALVEZ\"\n",
        "- A segunda linha DEVE comeÃ§ar exatamente com:\n",
        "  \"Justificativa:\"\n",
        "- A decisÃ£o final sempre deve ser de um mÃ©dico.\n",
        "\n",
        "PROIBIÃ‡Ã•ES ABSOLUTAS:\n",
        "- NÃƒO inclua rÃ³tulos internos, nomes tÃ©cnicos ou palavras como:\n",
        "  \"assistant.\", \"analysis\", \"context\".\n",
        "- NÃƒO repita a pergunta.\n",
        "- NÃƒO inclua texto fora das duas linhas.\n",
        "- NÃ£o responda em inglÃªs.\n",
        "- NÃ£o receite tratamentos ou medicamentos.\n",
        "\"\"\"\n",
        "\n",
        "def build_prompt(question: str, context: str) -> str:\n",
        "    return f\"\"\"\n",
        "            {SYSTEM_PROMPT}\n",
        "\n",
        "            Pergunta:\n",
        "            {question}\n",
        "\n",
        "            Contexto cientÃ­fico:\n",
        "            {context}\n",
        "\n",
        "            Resposta:\n",
        "            \"\"\".strip()\n",
        "\n",
        "def extract_final_answer(raw_text: str) -> str:\n",
        "    if not raw_text:\n",
        "        return \"\"\n",
        "\n",
        "    # Remove tudo antes de \"Resposta:\"\n",
        "    if \"Resposta:\" in raw_text:\n",
        "        raw_text = raw_text.split(\"Resposta:\", 1)[-1]\n",
        "\n",
        "    raw_text = raw_text.strip()\n",
        "\n",
        "    # Captura decisÃ£o e justificativa mesmo se estiverem na mesma linha\n",
        "    match = re.search(\n",
        "        r\"(DecisÃ£o:\\s*(SIM|NÃƒO|TALVEZ))\\s*(Justificativa:\\s*.+)\",\n",
        "        raw_text,\n",
        "        re.IGNORECASE | re.DOTALL\n",
        "    )\n",
        "\n",
        "    if not match:\n",
        "        return \"\"\n",
        "\n",
        "    decision = match.group(1).strip()\n",
        "    justification = match.group(3).strip()\n",
        "\n",
        "    return f\"{decision}\\n{justification}\"\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    question: str\n",
        "    context: str\n",
        "    answer: str\n",
        "\n",
        "def assistant_node(state: AgentState):\n",
        "    prompt = build_prompt(state[\"question\"], state[\"context\"])\n",
        "    response = llm.invoke(prompt).strip()\n",
        "    response = extract_final_answer(response)\n",
        "\n",
        "    if not validate_output(response):\n",
        "        response = \"DecisÃ£o: TALVEZ\\nJustificativa: O contexto fornecido Ã© insuficiente.\"\n",
        "\n",
        "    return {\"answer\": response}"
      ],
      "metadata": {
        "id": "gkJm-OeAqnFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRAFO"
      ],
      "metadata": {
        "id": "ToIwa7Cpqj6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "builder = StateGraph(AgentState)\n",
        "builder.add_node(\"assistant\", assistant_node)\n",
        "builder.set_entry_point(\"assistant\")\n",
        "builder.add_edge(\"assistant\", END)\n",
        "\n",
        "graph = builder.compile()"
      ],
      "metadata": {
        "id": "Yaw-ttsrxpL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VALIDA SAÃDA"
      ],
      "metadata": {
        "id": "RUW37Kl8qFkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_output(raw_text: str) -> bool:\n",
        "\n",
        "    extracted = raw_text\n",
        "\n",
        "    if not extracted:\n",
        "        return False\n",
        "\n",
        "    lines = extracted.splitlines()\n",
        "\n",
        "    if len(lines) != 2:\n",
        "        return False\n",
        "\n",
        "    if not lines[0].startswith(\"DecisÃ£o:\"):\n",
        "        return False\n",
        "\n",
        "    if not any(x in lines[0] for x in [\"SIM\", \"NÃƒO\", \"TALVEZ\"]):\n",
        "        return False\n",
        "\n",
        "    if not lines[1].startswith(\"Justificativa:\"):\n",
        "        return False\n",
        "\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "CKyogWYcoAhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TESTE DE EXECUÃ‡ÃƒO"
      ],
      "metadata": {
        "id": "JCaM1uIGqv2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_CONTEXT = (\n",
        "    \"O contexto a seguir representa evidÃªncias mÃ©dico-cientÃ­ficas consolidadas, \"\n",
        "    \"baseadas em estudos clÃ­nicos, revisÃµes sistemÃ¡ticas e diretrizes reconhecidas. \"\n",
        "    \"As informaÃ§Ãµes refletem consensos amplamente aceitos na literatura cientÃ­fica \"\n",
        "    \"e devem ser utilizadas exclusivamente para responder Ã  pergunta apresentada.\"\n",
        ")\n",
        "\n",
        "result = graph.invoke({\n",
        "     \"question\": \"O uso diÃ¡rio de protetor solar previne o cÃ¢ncer de pele em pessoas com alto risco de desenvolver a doenÃ§a?\",\n",
        "     \"context\": f\"{DEFAULT_CONTEXT}\"\n",
        "})\n",
        "\n",
        "print(result[\"answer\"])\n"
      ],
      "metadata": {
        "id": "fR3azva-qzKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TESTE DO CHAT ASSISTENTE"
      ],
      "metadata": {
        "id": "OiT8-jR62WYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, List\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "class ChatState(TypedDict):\n",
        "    messages: List[BaseMessage]\n",
        "\n",
        "#---------------------------------------------------------\n",
        "\n",
        "def assistant_node(state: ChatState) -> ChatState:\n",
        "    question = state[\"messages\"][-1].content\n",
        "\n",
        "    prompt = build_prompt(question, DEFAULT_CONTEXT)\n",
        "    response = llm.invoke(prompt).strip()\n",
        "    response = extract_final_answer(response)\n",
        "\n",
        "    return {\n",
        "        \"messages\": state[\"messages\"] + [AIMessage(content=response)]\n",
        "    }\n",
        "\n",
        "#----------------------------------------------------------\n",
        "\n",
        "def run_assistant_chat():\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ðŸ©º Assistente MÃ©dico-CientÃ­fico\")\n",
        "    print(\"Digite sua pergunta ou 'sair' para encerrar.\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    state = {\"messages\": []}\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nðŸ‘¤ Pergunta: \").strip()\n",
        "\n",
        "        if user_input.lower() in {\"sair\", \"exit\", \"quit\"}:\n",
        "            print(\"\\nðŸ‘‹ Encerrando o assistente.\")\n",
        "            break\n",
        "\n",
        "        state[\"messages\"].append(HumanMessage(content=user_input))\n",
        "\n",
        "        result = app.invoke(state)\n",
        "\n",
        "        answer = result[\"messages\"][-1].content.strip()\n",
        "\n",
        "        answer = extract_final_answer(answer)\n",
        "\n",
        "        # Garantia mÃ­nima de saÃ­da limpa\n",
        "        lines = [l for l in answer.splitlines() if l.strip()]\n",
        "        print(\"\\nðŸ¤– Resposta:\")\n",
        "        if len(lines) >= 2:\n",
        "            print(lines[0])\n",
        "            print(lines[1])\n",
        "        else:\n",
        "            print(answer)\n",
        "\n",
        "        state = result\n",
        "\n",
        "\n",
        "graph = StateGraph(ChatState)\n",
        "\n",
        "graph.add_node(\"assistant\", assistant_node)\n",
        "graph.set_entry_point(\"assistant\")\n",
        "graph.add_edge(\"assistant\", END)\n",
        "\n",
        "app = graph.compile()\n",
        "\n",
        "\n",
        "run_assistant_chat()\n"
      ],
      "metadata": {
        "id": "GqNNpw8J2eRm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}